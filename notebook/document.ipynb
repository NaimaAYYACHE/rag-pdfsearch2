{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded4d72",
   "metadata": {},
   "source": [
    "#### data Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db68c5",
   "metadata": {},
   "source": [
    "### Document   structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32dc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a514e821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'page': 1, 'author': 'naima AYYACHE', 'date_created': '2025-11-18'}, page_content='this is the main text content i am using to create RAG ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc =  Document (\n",
    "    page_content= \"this is the main text content i am using to create RAG \" , \n",
    "    metadata= {\n",
    "        \"source\":\"example.txt\" , \n",
    "        \"page\":1,\n",
    "        \"author\":\"naima AYYACHE\",\n",
    "        \"date_created\":\"2025-11-18\"\n",
    "    }\n",
    "\n",
    ")\n",
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf01073",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file \n",
    "import os \n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4b9625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple text files created\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "for filepath, content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f :\n",
    "        f.write(content)\n",
    "print(\"simple text files created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4bc031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "#from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4e5cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "## load all text files from the directory\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\" , \n",
    "    glob=\"*.txt\", ## Pattren to match files \n",
    "    loader_cls=TextLoader,  # the loader class to use \n",
    "    loader_kwargs={\"encoding\":\"utf-8\"},\n",
    ")\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b047560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 0}, page_content='Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was introduced. This idea is named the attention mechanism, and it\\nhas gone through a long development period. Today, many works have been\\ndevoted to this idea in a variety of tasks. Remarkable performance has re-\\ncently been demonstrated. The goal of this paper is to provide an overview\\nfrom the early work on searching for ways to implement attention idea with\\nneural networks until the recent trends. This review emphasizes the impor-\\ntant milestones during this progress regarding diﬀerent tasks. By this way,\\nthis study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.\\nKeywords Attention mechanism · Neural networks · Deep learning · Survey\\n1 Introduction\\nHuman eye sees the world in an interesting way. We suppose as if we see the\\nentire scene at once, but this is an illusion created by the subconscious part\\nof our brain [1]. According to the Scanpath theory [2,3], when the human eye\\nlooks at an image, it can see only a small patch in high resolution. This small\\npatch is called the fovea. It can see the rest of the image in low resolution which\\nis called the periphery. To recognize the entire scene, the eye performs feature\\nextraction based on the fovea. The eye is moved to diﬀerent parts of the image\\nuntil the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner\\nDepartment of Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium\\nTel.: +32-16710471\\nE-mail: derya.soydaner@kuleuven.be\\narXiv:2204.13154v1  [cs.LG]  27 Apr 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 1}, page_content='2\\nDerya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system. Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visual\\nﬁeld [5]. It seeks answers for two main questions: What and where to look?\\nVisual attention has been extensively studied in psychology and neuroscience;\\nfor reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\\nmodeling eye movements [11,12,13,14]. These studies have been a source of\\ninspiration for many artiﬁcial intelligence tasks. It has been discovered that\\nthe attention idea is useful from image recognition to machine translation.\\nTherefore, diﬀerent types of attention mechanisms inspired from the human\\nvisual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these\\nmechanisms have been integrated into neural networks for a long time.\\nThis survey is about the journey of attention mechanisms used with neu-\\nral networks. Researchers have been investigating ways to strengthen neural\\nnetwork architectures with attention mechanisms for many years. The pri-\\nmary aim of these studies is to reduce computational burden and to improve\\nthe model performance as well. Previous work reviewed the attention mecha-\\nnisms from diﬀerent perspectives [15], or examined them in context of natural\\nlanguage processing (NLP) [16,17]. However, in this study, we examine the\\ndevelopment of attention mechanisms over the years, and recent trends. We\\nbegin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-\\ntention mechanisms. One of them is the Transformer, which is used for many\\nstudies including the GPT-3 language model [18], goes beyond convolutions\\nand recurrence by replacing them with only attention layers [19]. Finally, we\\ndiscuss how much more can we move forward, and what’s next?\\n2 From the Late 1980s to Early 2010s: The Attention Awakens\\nThe ﬁrst attempts at adapting attention mechanisms to neural networks go\\nback to the late 1980s. One of the early studies is the improved version of\\nthe Neocognitron [20] with selective attention [21]. This study is then mod-\\niﬁed to recognize and segment connected characters in cursive handwriting\\n[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-\\ntecture named Signal Channelling Attentional Network (SCAN) is presented\\nfor attentional scanning [23].\\nEarly work on improving the attention idea for neural networks includes\\na variety of tasks such as target detection [24]. In another study, a visual at-\\ntention system extracts regions of interest by combining the bottom-up and\\ntop-down information from the image [25]. A recognition model based on se-\\nlective attention which analyses only a small part of the image at each step,\\nand combines results in time is described [4]. Besides, a model based on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 2}, page_content='Attention Mechanism in Neural Networks:\\n3\\nconcept of selective tuning is proposed [26]. As the years go by, several studies\\nthat use the attention idea in diﬀerent ways have been presented for visual\\nperception and recognition [27,28,29,30].\\nBy the 2000s, the studies on making attention mechanisms more useful for\\nneural networks continued. In the early years, a model that integrates an at-\\ntentional orienting where pathway and an object recognition what pathway is\\npresented [31]. A computational model of human eye movements is proposed\\nfor an object class detection task [32]. A serial model is presented for visual pat-\\ntern recognition gathering Markov models and neural networks with selective\\nattention on the handwritten digit recognition and face recognition problems\\n[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea\\nis used for object recognition [34], and the analysis of a scene [35]. An inter-\\nesting study proposes to learn sequential attention in real-world visual object\\nrecognition using a Q-learner [36]. Besides, a computational model of visual\\nselective attention is described to automatically detect the most relevant parts\\nof a color picture displayed on a television screen [37]. The attention idea is\\nalso used for identifying and tracking objects in multi-resolution digital video\\nof partially cluttered environments [38].\\nIn 2010, the ﬁrst implemented system inspired by the fovea of human retina\\nwas presented for image classiﬁcation [39]. This system jointly trains a re-\\nstricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-\\nmultaneous object tracking and recognition that is driven by gaze data [40].\\nBy taking advantage of reinforcement learning, a novel recurrent neural net-\\nwork (RNN) is described for image classiﬁcation [41]. Deep Attention Selective\\nNetwork (DasNet), a deep neural network with feedback connections that are\\nlearned through reinforcement learning to direct selective attention to certain\\nfeatures extracted from images, is presented [42]. Additionally, a deep learning\\nbased framework using attention has been proposed for generative modeling\\n[43].\\n3 2015: The Rise of Attention\\nIt can be said that 2015 is the golden year of attention mechanisms. Because\\nthe number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for\\nneural machine translation (NMT) [44]. As it is known, most of the NMT\\nmodels belong to a family of encoder-decoders [45,46], with an encoder and a\\ndecoder for each language. However, compressing all the necessary information\\nof a source sentence into a ﬁxed-length vector is an important disadvantage of\\nthis encoder-decoder approach. This usually makes it diﬃcult for the neural\\nnetwork to capture all the semantic details of a very long sentence [1].\\nThe idea that [44] introduced is an extension to the conventional NMT\\nmodels. This extension is composed of an encoder and decoder as shown in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 3}, page_content='4\\nDerya Soydaner\\nFig. 1 The extension to the conventional NMT models that is proposed by [44]. It generates\\nthe t-th target word yt given a source sentence (x1, x2, ..., xT ).\\nFig 1. The ﬁrst part, encoder, is a bidirectional RNN (BiRNN) [47] that takes\\nword vectors as input. The forward and backward states of BiRNN are com-\\nputed. Then, an annotation aj for each word xj is obtained by concatenating\\nthese forward and backward hidden states. Thus, the encoder maps the input\\nsentence to a sequence of annotations (a1, ..., aTx). By using a BiRNN rather\\nthan conventional RNN, the annotation of each word can summarize both\\nthe preceding words and the following words. Besides, the annotation aj can\\nfocus on the words around xj because of the inherent nature of RNNs that\\nrepresenting recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as\\nin Eq. (1). This neural network f is deﬁned as an alignment model that can\\nbe jointly trained with the proposed architecture. In order to reduce compu-\\ntational burden, a multilayer perceptron (MLP) with a single hidden layer is\\nproposed as f. This alignment model tells us about the relation between the\\ninputs around position j and the output at position i. By this way, the decoder\\napplies an attention mechanism. As it is seen in Eq. (2), the αij is the output\\nof softmax function:\\neij = f(hi−1, aj)\\n(1)\\nαij =\\nexp(eij)\\nPTx\\nk=1 exp(eik)\\n(2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 4}, page_content='Attention Mechanism in Neural Networks:\\n5\\nHere, the probability αij determines the importance of annotation aj with\\nrespect to the previous hidden state hi−1. Finally, the context vector ci is\\ncomputed as a weighted sum of these annotations as follows [44]:\\nci =\\nTx\\nX\\nj=1\\nαijaj\\n(3)\\nBased on the decoder state, the context and the last generated word, the\\ntarget word yt is predicted. In order to generate a word in a translation, the\\nmodel searches for the most relevant information in the source sentence to\\nconcentrate. When it ﬁnds the appropriate source positions, it makes the pre-\\ndiction. By this way, the input sentence is encoded into a sequence of vectors\\nand a subset of these vectors is selected adaptively by the decoder that is rel-\\nevant to predicting the target [44]. Thus, it is no longer necessary to compress\\nall the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning\\n[48]. Diﬀerent from the previous study [44], it uses a deep convolutional neural\\nnetwork (CNN) as an encoder. This architecture is an extension of the neural\\nnetwork [49] that encodes an image into a compact representation, followed by\\nan RNN that generates a corresponding sentence. Here, the annotation vectors\\nai ∈RD are extracted from a lower convolutional layer, each of which is a D-\\ndimensional representation corresponding to a part of the image. Thus, the\\ndecoder selectively focuses on certain parts of an image by weighting a subset\\nof all the feature vectors [48]. This extended architecture uses attention for\\nsalient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at\\ntime t. The weight αi of each annotation vector is computed similar to Eq. (2),\\nwhereas its associated energy is computed similar to Eq. (1) by using an MLP\\nconditioned on the previous hidden state ht−1. The remarkable point of this\\nstudy is a new mechanism φ that computes ct from the annotation vectors ai\\ncorresponding to the features extracted at diﬀerent image locations:\\nct = φ(\\n\\x08\\nai\\n\\t\\n,\\n\\x08\\nαi\\n\\t\\n)\\n(4)\\nThe deﬁnition of the φ function causes two variants of attention mecha-\\nnisms: The hard (stochastic) attention mechanism is trainable by maximizing\\nan approximate variational lower bound, i.e., by REINFORCE [50]. On the\\nother side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable\\nst, and uses it to decide where to focus attention when generating the t-th\\nword. When the hard attention is applied, the attention locations are con-\\nsidered as intermediate latent variables. It assigns a multinoulli distribution\\nparametrized by αi, and ct becomes a random variable. Here, st,i is deﬁned\\nas a one-hot variable which is set to 1 if the i-th location is used to extract\\nvisual features [48]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 5}, page_content='6\\nDerya Soydaner\\np(st,i = 1|sj<t, a) = αt,i\\n(5)\\nct =\\nX\\ni\\nst,iai\\n(6)\\nWhereas learning hard attention requires sampling the attention location\\nst each time, the soft attention mechanism computes a weighted annotation\\nvector similar to [44] and takes the expectation of the context vector ct directly:\\nEp(st|α)[ct] =\\nL\\nX\\ni=1\\nαt,iai\\n(7)\\nFurthermore, in training the deterministic version of the model, an alterna-\\ntive method namely doubly stochastic attention, is proposed with an additional\\nconstraint added to the training objective to encourage the model to pay equal\\nattention to all parts of the image.\\nThe third study should be emphasized presents two classes of attention\\nmechanisms for NMT: the global attention that always attends to all source\\nwords, and the local attention that only looks at a subset of source words at\\na time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the\\nlocal one selectively focuses on a small window of context. In global attention, a\\nvariable-length alignment vector is derived similar to Eq. (2). Here, the current\\ntarget hidden state ht is compared with each source hidden state ¯hs by using a\\nscore function instead of the associated energy eij. Thus, the alignment vector\\nwhose size equals the number of time steps on the source side is derived.\\nGiven the alignment vector as weights, the context vector ct is computed as\\nthe weighted average over all the source hidden states. Here, score is referred\\nas a content-based function, and three diﬀerent alternatives are considered [51].\\nOn the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The\\nlocal attention selectively focuses on a small window of context, and obtains\\nthe alignment vector from the current target state ht and the source states ¯hs\\nin the window [51].\\nThe introduction of these novel mechanisms in 2015 triggered the rise of\\nattention for neural networks. Based on the proposed attention mechanisms,\\nsigniﬁcant research has been conducted in a variety of tasks. In order to imag-\\nine the attention idea in neural networks better, two visual examples are shown\\nin Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows\\nhow the attention mechanisms can be used for visual question answering [52].\\nBoth examples demonstrate how attention mechanisms focus on parts of input\\nimages.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 6}, page_content='Attention Mechanism in Neural Networks:\\n7\\nFig. 2 Examples of the attention mechanism in visual. (Top) Attending to the correct\\nobject in neural image caption generation [48]. (Bottom) Visualization of original image\\nand question pairs, and co-attention maps namely word-level, phrase-level and question-\\nlevel, respectively [52].\\n4 2015-2016: Attack of the Attention\\nDuring two years from 2015, the attention mechanisms were used for diﬀerent\\ntasks, and novel neural network architectures were presented applying these\\nmechanisms. After the memory networks [53] that require a supervision signal\\ninstructing them how to use their memory cells, the introduction of the neural\\nTuring machine [54] allows end-to-end training without this supervision signal,\\nvia the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.\\nIn these years, an attention mechanism called self-attention, sometimes\\ncalled intra-attention, was successfully implemented within a neural network\\narchitecture namely Long Short-Term Memory-Networks (LSTMN) [56]. It\\nmodiﬁes the standard LSTM structure by replacing the memory cell with a\\nmemory network [53]. This is because memory networks have a set of key\\nvectors and a set of value vectors, whereas LSTMs maintain a hidden vector\\nand a memory vector [56]. In contrast to attention idea in [44], memory and\\nattention are added within a sequence encoder in LSTMN. In order to compute\\na representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].\\nMany attention-based models have been proposed for neural image cap-\\ntioning [58], abstractive sentence summarization [59], speech recognition [60,\\n61], automatic video captioning [62], neural machine translation [63], and rec-\\nognizing textual entailment [64]. Diﬀerent attention-based models perform vi-\\nsual question answering [65,66,67]. An attention-based CNN is presented for\\nmodeling sentence pairs [68]. A recurrent soft attention based model learns to\\nfocus selectively on parts of the video frames and classiﬁes videos [69].\\nOn the other side, several neural network architectures have been pre-\\nsented in a variety of tasks. For instance, Stacked Attention Network (SAN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 7}, page_content='8\\nDerya Soydaner\\nis described for image question answering [70]. Deep Attention Recurrent Q-\\nNetwork (DARQN) integrates soft and hard attention mechanisms into the\\nstructure of Deep Q-Network (DQN) [71]. Wake-Sleep Recurrent Attention\\nModel (WS-RAM) speeds up the training time for image classiﬁcation and\\ncaption generation tasks [72]. alignDRAW model, an extension of the Deep\\nRecurrent Attention Writer (DRAW) [73], is a generative model of images\\nfrom captions using a soft attention mechanism [74]. Generative Adversarial\\nWhat-Where Network (GAWWN) synthesizes images given instructions de-\\nscribing what content to draw in which location [75].\\n5 The Transformer: Return of the Attention\\nAfter the proposed attention mechanisms in 2015, researchers published stud-\\nies that mostly modifying or implementing them to diﬀerent tasks. However,\\nin 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great\\nresults on two machine translation tasks in addition to English constituency\\nparsing. The most impressive point about this architecture is that it contains\\nneither recurrence nor convolution. The Transformer performs well by replac-\\ning the conventional recurrent layers in encoder-decoder architecture used for\\nNMT with self-attention.\\nThe Transformer is composed of encoder-decoder stacks each of which has\\nsix identical layers within itself. In Fig. 3, one encoder-decoder stack is shown\\nto illustrate the model [19]. Each stack includes only attention mechanisms\\nand feedforward neural networks. As this architecture does not include any\\nrecurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.\\nThe calculations of self-attention are slightly diﬀerent from the mechanisms\\ndescribed so far in this paper. It uses three vectors namely query, key and\\nvalue for each word. These vectors are computed by multiplying the input with\\nweight matrices Wq, Wk and Wv which are learned during training. In general,\\neach value is weighted by a function of the query with the corresponding key.\\nThe output is computed as a weighted sum of the values. Based on this idea,\\ntwo attention mechanisms are proposed: In the ﬁrst one, called scaled dot-\\nproduct attention, the dot products of the query with all keys are computed\\nas given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the\\nsoftmax function, thus the weights for the values are obtained. Finally each\\nsoftmax score is multiplied with the value as given in Eq. (8). The authors\\npropose computing the attention on a set of queries simultaneously by taking\\nqueries and keys of dimension dk, and values of dimension dv as inputs. The\\nkeys, queries and values are packed together into matrices K, Q and V. Finally,\\nthe output matrix is obtained as follows [19]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 8}, page_content='Attention Mechanism in Neural Networks:\\n9\\nFig. 3 The Transformer architecture and the attention mechanisms it uses in detail [19].\\n(Left) The Transformer with one encoder-decoder stack. (Center) Multi-head attention.\\n(Right) Scaled dot-product attention.\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(8)\\nThis calculation is performed by every word against the other words. This\\nleads to having values of each word relative to each other. For instance, if\\nthe word x2 is not relevant for the word x1, then the softmax score gives low\\nprobability scores. As a result, the corresponding value is decreased. This leads\\nto an increase in the value of relevant words, and those of others decrease. In\\nthe end, every word obtains a new value for itself.\\nAs seen from Fig. 3, the Transformer model does not directly use scaled\\ndot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,\\nlinearly projects the queries, keys and values h times with diﬀerent, learned\\nlinear projections to dq, dk and dv dimensions, respectively [19]. The attention\\nfunction is performed in parallel on each of these projected versions of queries,\\nkeys and values, i.e., heads. By this way, dv-dimensional output values are\\nobtained. In order to get the ﬁnal values, they are concatenated and projected\\none last time as shown in the center of Fig. 3. By this way, the self-attention is\\ncalculated multiple times using diﬀerent sets of query, key and value vectors.\\nThus, the model can jointly attend to information at diﬀerent positions [19]:\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\n(9)\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is\\napplied ﬁrst to ensure that only previous word embeddings are used when\\ntrying to predict the next word in the sentence. Therefore, the embeddings\\nthat shouldn’t be seen by the decoder are masked by multiplying with zero.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 9}, page_content='10\\nDerya Soydaner\\nAn interesting study examines the contribution made by individual atten-\\ntion heads in the encoder [76]. Also, there is an evaluation of the eﬀects of\\nself-attention on gradient propagation in recurrent networks [77]. For a deeper\\nanalysis of multi-head self-attention mechanism from a theoretical perspective\\nsee [78].\\nSelf-attention has been used successfully in a variety of tasks including\\nsentence embedding [79] and abstractive summarization [80]. It is shown that\\nself-attention can lead to improvements to discriminative constituency parser\\n[81], and speech recognition as well [82,83]. Also, the listen-attend-spell model\\n[84] has been improved with the self-attention for acoustic modeling [85].\\nAs soon as these self-attention mechanisms were proposed, they have been\\nincorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-\\nmains with the aid of self-attention mechanism [86]. Novel self-attention neural\\nmodels are proposed for cross-target stance classiﬁcation [87] and NMT [88].\\nAnother study points out that a fully self-attentional model can reach com-\\npetitive predictive performance on ImageNet classiﬁcation and COCO object\\ndetection tasks [89]. Besides, developing novel attention mechanisms has been\\ncarried out such as area attention, a novel mechanism that can be used along\\nmulti-head attention [90]. It attends to areas in the memory by deﬁning the\\nkey of an area as the mean vector of the key of each item, and deﬁning the\\nvalue as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks\\n(SAGANs) [92] introduce a self-attention mechanism into convolutional GANs.\\nDiﬀerent from the traditional convolutional GANs, SAGAN generates high-\\nresolution details using cues from all feature locations. Similarly, Attentional\\nGenerative Adversarial Network (AttnGAN) is presented for text to image\\ngeneration [93]. On the other side, a machine reading and question answering\\narchitecture called QANet [94] is proposed without any recurrent networks. It\\nuses self-attention to learn the global interaction between each pair of words\\nwhereas convolution captures the local structure of the text. In another study,\\nGated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces\\nattentive group convolutions with a generalization of visual self-attention [96].\\nA deep transformer model is implemented for language modeling over long\\nsequences [97].\\n5.1 Self-attention variants\\nIn recent years, self-attention has become an important research direction\\nwithin the deep learning community. Self-attention idea has been examined\\nin diﬀerent aspects. For example, self-attention is handled in a multi-instance\\nlearning framework [98]. The idea of Sparse Adaptive Connection (SAC) is\\npresented for accelerating and structuring self-attention [99]. The research on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 10}, page_content='Attention Mechanism in Neural Networks:\\n11\\nTable 1 Summary of Notation\\nSymbol\\nDeﬁnition\\na\\nannotation\\nc\\ncontext vector\\nα\\nweight\\ne\\nenergy\\nf\\nfeedforward neural network\\nh\\nhidden state\\nφ\\nhard (stochastic) / soft (deterministic) attention\\ns\\nlocation variable\\np\\nsource position\\nK, Q, V\\nkeys, queries and values matrices, respectively\\nWq, Wk, Wv\\nweight matrices for queries, keys and values, respectively\\nimproving self-attention continues as well [100,101,102]. Besides, based on the\\nself-attention mechanisms proposed in the Transformer, important studies that\\nmodify the self-attention have been presented. Some of the most recent and\\nprominent studies are summarized below.\\nRelation-aware self-attention It extends the self-attention mechanism by\\nregarding representations of the relative positions, or distances between se-\\nquence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent\\nthe edge between two inputs. It provides learning two distinct edge represen-\\ntations that can be shared across attention heads without requiring additional\\nlinear transformations.\\nDirectional self-attention (DiSA) A novel neural network architecture\\nfor learning sentence embedding named Directional Self-Attention Network\\n(DiSAN) [104] uses directional self-attention followed by a multi-dimensional\\nattention mechanism. Instead of computing a single importance score for each\\nword based on the word embedding, multi-dimensional attention computes a\\nfeature-wise score vector for each token. To extend this mechanism to the self-\\nattention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.\\nThe second one, called multi-dimensional ‘source2token’ self-attention com-\\npresses the sequence into a vector [104]. On the other side, directional self-\\nattention produces context-aware representations with temporal information\\nencoded by using positional masks. By this way, directional information is en-\\ncoded. First, the input sequence is transformed to a sequence of hidden states\\nby a fully connected layer. Then, multi-dimensional token2token self-attention\\nis applied to these hidden states. Hence, context-aware vector representations\\nare generated for all elements from the input sequence.\\nReinforced self-attention (ReSA) A sentence-encoding model named Re-\\ninforced Self-Attention Network (ReSAN) uses reinforced self-attention (ReSA)\\nthat integrates soft and hard attention mechanisms into a single model. ReSA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 11}, page_content='12\\nDerya Soydaner\\nselects a subset of head tokens, and relates each head token to a small sub-\\nset of dependent tokens to generate their context-aware representations [105].\\nFor this purpose, a novel hard attention mechanism called reinforced sequence\\nsampling (RSS), which selects tokens from an input sequence in parallel and\\ntrained via policy gradient, is proposed. Given an input sequence, RSS gener-\\nates an equal-length sequence of binary random variables that indicates both\\nthe selected and discarded ones. On the other side, the soft attention provides\\nreward signals back for training the hard attention. The proposed RSS pro-\\nvides a sparse mask to self-attention. ReSA uses two RSS modules to extract\\nthe sparse dependencies between each pair of selected tokens.\\nOuter product attention (OPA) Self-Attentive Associative Memory (SAM)\\nis a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers\\nusing element-wise multiplication, outer product, and tanh function instead of\\nsoftmax.\\nBidirectional block self-attention (Bi-BloSA) Another mechanism, bidi-\\nrectional block self-attention (Bi-BloSA) which is simply a masked block self-\\nattention (mBloSA) with forward and backward masks to encode the tempo-\\nral order information is presented [107]. Here, mBloSA is composed of three\\nparts from its bottom to top namely intra-block self-attention, inter-block\\nself-attention and the context fusion. It splits a sequence into several length-\\nequal blocks, and applies an intra-block self-attention to each block indepen-\\ndently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-\\nory compared to a single one applied to the whole sequence. Finally, a feature\\nfusion gate combines the outputs of intra-block and inter-block self-attention\\nwith the original input, to produce the ﬁnal context-aware representations of\\nall tokens.\\nFixed multi-head attention The ﬁxed multi-head attention proposes ﬁxing\\nthe head size of the Transformer in the aim of improving the representation\\npower [108]. This study emphasizes its importance by setting the head size of\\nattention units to input sequence length.\\nSparse sinkhorn attention It is based on the idea of diﬀerentiable sorting\\nof internal representations within the self-attention module [109]. Instead of\\nallowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.\\nThus, tokens that may be far apart in the unsorted sequence can be considered.\\nAdditionally, a variant of this mechanism named SortCut sinkhorn attention\\napplies a post-sorting truncation of the input sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 12}, page_content='Attention Mechanism in Neural Networks:\\n13\\nAdaptive attention span Adaptive attention span is proposed as an alter-\\nnative to self-attention [110]. It learns the attention span of each head inde-\\npendently. To this end, a masking function inspired by [111] is used to control\\nthe attention span for each head. The purpose of this novel mechanism is to\\nreduce the computational burden of the Transformer. Additionally, dynamic\\nattention span approach is presented to dynamically change the attention span\\nbased on the current input as an extension [51,112].\\n5.2 Transformer variants\\nDiﬀerent from developing novel self-attention mechanisms, several studies have\\nbeen published in the aim of improving the performance of the Transformer.\\nThese studies mostly modify the model architecture. For instance, an addi-\\ntional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to\\nimprove Transformer optimization [114]. A novel positional encoding scheme\\nis used to extend the Transformer to tree-structured data [115]. Investigating\\nmodel size by handling Transformer width and depth for eﬃcient training is\\nalso an active research area [116]. Transformer is used in reinforcement learn-\\ning settings [117,118,119] and for time series forecasting in adversarial training\\nsetting [120].\\nBesides, many Transformer variants have been presented in the recent past.\\nCOMmonsEnse Transformer (COMET) is introduced for automatic construc-\\ntion of commonsense knowledge bases [121]. Evolved Transformer applies neu-\\nral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].\\nCrossTransformer takes a small number of labeled images and an unlabeled\\nquery, and computes distances between spatially-corresponding features to in-\\nfer class membership [124]. DEtection TRansformer (DETR) is a new design\\nfor object detection systems [125], and Deformable DETR is an improved ver-\\nsion that achieves better performance in less time [126]. FLOw-bAsed Trans-\\nformER (FLOATER) emphasizes the importance of position encoding in the\\nTransformer, and models the position information via a continuous dynamical\\nmodel [127]. Disentangled Context (DisCo) Transformer simultaneously gener-\\nates all tokens given diﬀerent contexts by predicting every word in a sentence\\nconditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative\\nmodeling [129].\\nRecent work has demonstrated signiﬁcant performance on NLP tasks. In\\nOpenAI GPT, there is a left-to-right architecture, where every token can only\\nattend to previous tokens in the self-attention layers of the Transformer [130].\\nGPT-2 [131] and GPT-3 [18] models have improved the progress. In addition\\nto these variants, some prominent Transformer-based models are summarized\\nbelow.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 13}, page_content='14\\nDerya Soydaner\\nUniversal Transformer A generalization of the Transformer model named\\nthe Universal Transformer [132] iteratively computes representations Ht at\\nstep t for all positions in the sequence in parallel. To this end, it uses the\\nscaled dot-product attention in Eq. (8) where d is the number of columns\\nof Q, K and V. In the Universal Transformer, the multi-head self-attention\\nwith k heads is used. The representations Ht is mapped to queries, keys and\\nvalues with aﬃne projections using learned parameter matrices W Q ∈ℜd×d/k,\\nW K ∈ℜd×d/k, W V ∈ℜd×d/k and W O ∈ℜd×d [132]:\\nMultiHead(Ht) = Concat(head1, ..., headk)W O\\n(10)\\nwhere headi = Attention(HtW Q\\ni , HtW K\\ni , HtW V\\ni )\\nImage Transformer Image Transformer [133] demonstrates that self-attention\\nbased models can also be well-suited for images instead of text. This Trans-\\nformer type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.\\nIts larger receptive ﬁelds allow the Image Transformer to signiﬁcantly improve\\nthe model performance on image generation as well as image super-resolution.\\nTransformer-XL This study aims to improve the ﬁxed-length context of the\\nTransformer [19] for language modeling. Transformer-XL [134] makes model-\\ning very long-term dependency possible by reusing the hidden states obtained\\nin previous segments. Hence, information can be propagated through the recur-\\nrent connections. In order to reuse the hidden states without causing temporal\\nconfusion, Transformer-XL uses relative positional encodings. Based on this\\narchitecture, a modiﬁed version named the Gated Transformer-XL (GTrXL)\\nis presented in the reinforcement learning setting [135].\\nTensorized Transformer Tensorized Transformer [136] compresses the multi-\\nhead attention in Transformer. To this end, it uses a novel self-attention model\\nmulti-linear attention with Block-Term Tensor Decomposition (BTD) [137]. It\\nbuilds a single-block attention based on the Tucker decomposition [138]. Then,\\nit uses a multi-linear attention constructed by a BTD to compress the multi-\\nhead attention mechanism. In Tensorized Transformer, the factor matrices are\\nshared across multiple blocks.\\nBERT The Bidirectional Encoder Representations from Transformers (BERT)\\naims to pre-train deep bidirectional representations from unlabeled text [139].\\nBERT uses a multilayer bidirectional Transformer as the encoder. Besides,\\ninspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-\\ndicts the original vocabulary id of the masked word based only on its context.\\nThis model can pre-train a deep bidirectional Transformer. In all layers, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 14}, page_content='Attention Mechanism in Neural Networks:\\n15\\npre-training is carried out by jointly conditioning on both left and right con-\\ntext. BERT diﬀers from the left-to-right language model pre-training from this\\naspect.\\nRecently, BERT model has been examined in detail. For instance, the be-\\nhaviour of attention heads are analysed [141]. Various methods have been\\ninvestigated for compressing [142,143], pruning [144], and quantization [145].\\nAlso, BERT model has been considered for diﬀerent tasks such as coreference\\nresolution [146]. A novel method is proposed in order to accelerate BERT\\ntraining [147].\\nFurthermore, various BERT variants have been presented. ALBERT aims\\nto increase the training speed of BERT, and presents two parameter reduction\\ntechniques [148]. Similarly, PoWER-BERT [149] is developed to improve the\\ninference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while\\nmaintaining accuracy [150]. In order to obtain better representations, Span-\\nBERT is proposed as a pre-training method [151]. As a robustly optimized\\nBERT approach, RoBERTa shows that BERT was signiﬁcantly undertrained\\n[152]. Also, DeBERTa improves RoBERTa using the disentangled attention\\nmechanism [153]. On the other side, DistilBERT shows that it is possible to\\nreach similar performances using much smaller language models pre-trained\\nwith knowledge distillation [154]. StructBERT proposes two novel lineariza-\\ntion strategies [155]. Q-BERT is introduced for quantizing BERT models [156],\\nBioBERT is for biomedical text mining [157], and RareBERT is for rare dis-\\nease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-\\nformer for various tasks, or making them more understandable. In one of the\\nmost recent studies, NLP becomes possible in the mobile setting with Lite\\nTransformer. It applies long-short range attention where some heads specialize\\nin the local context modeling while the others specialize in the long-distance\\nrelationship modeling [159]. A deep and light-weight Transformer DeLighT\\n[160] and a hypernetwork-based model namely HyperGrid Transformers [161]\\nperform with fewer parameters. Graph Transformer Network is introduced\\nfor learning node representations on heterogeneous graphs [162] and diﬀerent\\napplications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured\\ntext data [165]. AttentionXML is a tree-based model for extreme multi-label\\ntext classiﬁcation [166]. Besides, attention mechanism is handled in a Bayesian\\nframework [167]. For a better understanding of Transformers, an identiﬁabil-\\nity analysis of self-attention weights is conducted in addition to presenting\\neﬀective attention to improve explanatory interpretations [168]. Lastly, Vision\\nTransformer (ViT) processes an image using a standard Transformer encoder\\nas used in NLP by interpreting it as a sequence of patches, and performs well\\non image classiﬁcation tasks [169].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 15}, page_content='16\\nDerya Soydaner\\n5.3 What about complexity?\\nAll these aforementioned studies undoubtedly demonstrate signiﬁcant success.\\nBut success not make one great. The Transformer also brings a very high\\ncomputational complexity and memory cost. The necessity of storing atten-\\ntion matrix to compute the gradients with respect to queries, keys and val-\\nues causes a non-negligible quadratic computation and memory requirements.\\nTraining the Transformer is a slow process for very long sequences because\\nof its quadratic complexity. There is also time complexity which is quadratic\\nwith respect to the sequence length. In order to improve the Transformer in\\nthis respect, recent studies have been conducted to improve this issue. One\\nof them is Linear Transformer which expresses the self-attention as a linear\\ndot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax\\nfunction in Eq. (8) to a feature map based dot-product attention. Its per-\\nformance is competitive with the vanilla Transformer architecture on image\\ngeneration and automatic speech recognition tasks while being faster during\\ninference. On the other side, FMMformers which use the idea of the fast multi-\\npole method (FMM) [171] outperform the linear Transformer by decomposing\\nthe attention matrix into near-ﬁeld and far-ﬁeld attention with linear time and\\nmemory complexity [172].\\nAnother suggestion made in response to the Transformer’s quadratic na-\\nture is The Reformer that replaces dot-product attention by one that uses\\nlocality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set\\nTransformer aims to reduce computation time of self-attention from quadratic\\nto linear by using an attention mechanism based on sparse Gaussian process\\nliterature [174]. Routing Transformer aims to reduce the overall complexity\\nof attention by learning dynamic sparse attention patterns by using routing\\nattention with clustering [175]. It applies k-means clustering to model sparse\\nattention matrices. At ﬁrst, queries and keys are assigned to clusters. The at-\\ntention scheme is determined by considering only queries and keys from the\\nsame cluster. Thus, queries are routed to keys belonging to the same cluster\\n[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-\\nputational burden [176]. It also shows the possibility of modeling sequences\\nof length one million or more by using self-attention in theory. In the Trans-\\nformer, all the attention heads with the softmax attention assign a non-zero\\nweight to all context words. Adaptively Sparse Transformer replaces softmax\\nwith α-entmax which is a diﬀerentiable generalization of softmax allowing\\nlow-scoring words to receive precisely zero weight [177]. By means of context-\\ndependent sparsity patterns, the attention heads become ﬂexible in the Adap-\\ntively Sparse Transformer. Random feature attention approximates softmax\\nattention with random feature methods [178]. Skyformer replaces softmax\\nwith a Gaussian kernel and adapts Nystr¨om method [179]. A sparse atten-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 16}, page_content='Attention Mechanism in Neural Networks:\\n17\\ntion mechanism named BIGBIRD aims to reduce the quadratic dependency\\nof Transformer-based models to linear [180]. Diﬀerent from the similar stud-\\nies, BIGBIRD performs well for genomics data alongside NLP tasks such as\\nquestion answering.\\nMusic Transformer [181] shows that self-attention can also be useful for\\nmodeling music. This study emphasizes the infeasibility of the relative po-\\nsition representations introduced by [103] for long sequences because of the\\nquadratic intermediate relative information in the sequence length. Therefore,\\nthis study presents an extended version of relative attention named relative\\nlocal attention that improves the relative attention for longer musical com-\\npositions by reducing its intermediate memory requirement to linear in the\\nsequence length. A softmax-free Transformer (SOFT) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead\\nof the dot-product similarity [182].\\nAdditionally, various approaches have been presented in Hierarchical Vi-\\nsual Transformer [183], Long-Short Transformer (Transformer-LS) [184], Per-\\nceiver [185], and Performer [186]. Image Transformer based on the cross-\\ncovariance matrix between keys and queries is applied [187], and a new vi-\\nsion Transformer is proposed [188]. Furthermore, a Bernoulli sampling atten-\\ntion mechanism decreases the quadratic complexity to linear [189]. A novel\\nlinearized attention mechanism performs well on object detection, instance\\nsegmentation, and stereo depth estimation [190]. A study shows that kernel-\\nized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long\\nsequences [191]. A linear uniﬁed nested attention mechanism namely Luna\\nuses two nested attention functions to approximate the softmax attention in\\nTransformer to achieve linear time and space complexity [192].\\n6 Concluding Remarks: A New Hope\\nInspired by the human visual system, the attention mechanisms in neural net-\\nworks have been developing for a long time. In this study, we examine this\\nduration beginning with its roots up to the present time. Some mechanisms\\nhave been modiﬁed, or novel mechanisms have emerged in this period. Today,\\nthis journey has reached a very important stage. The idea of incorporating\\nattention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family\\nmodels have become a new hope for more advanced models. These promising\\nprogress bring the questions whether the attention could help further devel-\\nopment, replace the popular neural network layers, or could be a better idea\\nthan the existing attention mechanisms? It is still an active research area and\\nmuch to learn we still have, but it is obvious that more powerful systems are\\nawaiting when neural networks and attention mechanisms join forces.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 17}, page_content='18\\nDerya Soydaner\\nConﬂict of interest\\nThe author declares that she has no conﬂict of interest.\\nReferences\\n1. I. Goodfellow, Y. Bengio, A. Courville, The MIT Press (2016)\\n2. D. Noton, L. Stark, Scientiﬁc American 224(6), 34 (1971)\\n3. D. Noton, L. Stark, Vision Research 11, 929 (1971)\\n4. E. Alpaydın, Advances in Neural Information Processing Systems 8 pp. 771–777 (1995)\\n5. S. Ahmad, Advances in Neural Information Processing Systems 4 pp. 420–427 (1991)\\n6. M. Posner, S. Petersen, Annual Review of Neuroscience 13(1), 25 (1990)\\n7. C. Bundesen, Psychological Review 97(4), 523 (1990)\\n8. R. Desimone, J. Duncan, Annual Review of Neuroscience 18(1), 193 (1995)\\n9. M. Corbetta, G. Shulman, Nature Reviews Neuroscience 3(3), 201 (2002)\\n10. S. Petersen, M. Posner, Annual Review of Neuroscience 35, 73 (2012)\\n11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)\\n13. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 105(2), 261 (1995)\\n14. J. Hoﬀman, B. Subramaniam, Perception and Psychophysics 57(6), 787 (1995)\\n15. S. Chaudhari, et al., ACM Transactions on Intelligent Systems and Technology (TIST)\\npp. 1–32 (2021)\\n16. A. Galassi, et al., IEEE Transactions on Neural Networks and Learning Systems (2020)\\n17. J. Lee, et al., ACM Transactions on Knowledge Discovery from Data (TKDD) 13(6),\\n1 (2019)\\n18. T. Brown, et al., Advances in Neural Information Processing Systems 33 pp. 1877–1901\\n(2020)\\n19. A. Vaswani, et al., Advances in Neural Information Processing Systems 30 pp. 5998–\\n6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)\\n23. E. Postma, H.V. den Herik, P. Hudson, Neural Networks 10(6), 993 (1997)\\n24. J. Schmidhuber, R. Huber, International Journal of Neural Systems pp. 125–134 (1991)\\n25. R. Milanese, et al., IEEE Computer Society Conference on Computer Vision and Pat-\\ntern Recoginition, Seattle, WA, USA pp. 781–785 (1994)\\n26. J. Tsotsos, et al., Artiﬁcial Intelligence 78(1-2), 507 (1995)\\n27. S. Culhane, J. Tsotsos, Proceedings of the 11th IAPR International Conference on\\nPattern Recognition, The Hague, Netherlands pp. 36–40 (1992)\\n28. D. Reisfeld, H. Wolfson, Y. Yeshurun, International Journal of Computer Vision 14(2),\\n119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)\\n31. F. Miau, L. Itti, Proceedings of the 23rd Annual International Conference of the IEEE\\nEngineering in Medicine and Biology Society, Istanbul, Turkey pp. 789–792 (2001)\\n32. W. Zhang, et al., Advances in Neural Information Processing Systems 19 pp. 1609–1616\\n(2006)\\n33. A. Salah, E. Alpaydın, L. Akarun, IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 24(3), 420 (2002)\\n34. D. Walther, et al., International Workshop on Biologically Motivated Computer Vision,\\nSpringer, Berlin, Heidelberg pp. 472–479 (2002)\\n35. K. Schill, et al., Journal of Electronic Imaging 10(1), 152 (2001)\\n36. L. Paletta, G. Fritz, C. Seifert, International Conference on Machine Learning (2005)\\n37. O.L. Meur, et al., IEEE Transactions on Pattern Analysis and Machine Intelligence\\n28(5), 802– (2006)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 18}, page_content='Attention Mechanism in Neural Networks:\\n19\\n38. S. Gould, et al., International Joint Conference on Artiﬁcial Intelligence (IJCAI) pp.\\n2115–2121 (2007)\\n39. H. Larochelle, G. Hinton, Advances in Neural Information Processing Systems 23 pp.\\n1243–1251 (2010)\\n40. L. Bazzani, et al., International Conference on Machine Learning (2011)\\n41. V. Mnih, et al., Advances in Neural Information Processing Systems 27 pp. 2204–2212\\n(2014)\\n42. M. Stollenga, et al., Advances in Neural Information Processing Systems 27 pp. 3545–\\n3553 (2014)\\n43. Y. Tang, N. Srivastava, R. Salakhutdinov, Advances in Neural Information Processing\\nSystems 27 (2014)\\n44. D. Bahdanau, K. Cho, Y. Bengio, International Conference on Learning Representa-\\ntions (2015)\\n45. I. Sutskever, O. Vinyals, Q. Le, Advances in Neural Information Processing Systems\\n27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)\\n47. M. Schuster, K. Paliwal, IEEE Transactions on Signal Processing 45(11), 2673 (1997)\\n48. K. Xu, et al., International Conference on Machine Learning pp. 2048–2057 (2015)\\n49. O. Vinyals, et al., In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition pp. 3156–3164 (2015)\\n50. R. Williams, Machine Learning 8(3-4), 229 (1992)\\n51. M.T. Luong, H.P..C. Manning, Proceedings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, Lisbon, Portugal pp. 1412–1421 (2015)\\n52. J. Lu, et al., Advances in Neural Information Processing Systems 29 (2016)\\n53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)\\n55. S. Sukhbaatar, et al., Advances in Neural Information Processing Systems 28 pp. 2440–\\n2448 (2015)\\n56. J. Cheng, L. Dong, M. Lapata, Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing pp. 551–561 (2016)\\n57. A. Parikh, et al., Proceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing, Austin, Texas pp. 2249–2255 (2016)\\n58. Q. You, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV pp. 4651–4659 (2016)\\n59. A. Rush, S. Chopra, J. Weston, Proceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)\\n61. J. Chorowski, et al., Advances in Neural Information Processing Systems 28 pp. 577–\\n585 (2015)\\n62. M. Zanﬁr, E. Marinoiu, C. Sminchisescu, In Asian Conference on Computer Vision,\\nSpringer, Cham pp. 104—-119 (2016)\\n63. Y. Cheng, et al., Proceedings of the 25th International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n64. T. Rockt International Conference on Learning Representations (2016)\\n65. Y. Zhu, et al., Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 4995–5004 (2016)\\n66. K. Chen, et al., arXiv preprint arXiv:1511.05960 (2015)\\n67. H. Xu, K. Saenko, In European Conference on Computer Vision pp. 451–466 (2016)\\n68. W. Yin, et al., Transactions of the Association for Computational Linguistics 4, 259\\n(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)\\n70. Z. Yang, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 21–29 (2016)\\n71. I. Sorokin, et al., arXiv preprint arXiv:1512.01693 (2015)\\n72. J. Ba, et al., Advances in Neural Information Processing Systems 28 pp. 2593–2601\\n(2015)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 19}, page_content='20\\nDerya Soydaner\\n73. K. Gregor, et al., International Conference on Machine Learning pp. 1462–1471 (2015)\\n74. E. Mansimov, et al., International Conference on Learning Representations (2016)\\n75. S. Reed, et al., Advances in Neural Information Processing Systems 29 pp. 217–225\\n(2016)\\n76. E. Voita, et al., In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 5797–5808 (2019)\\n77. G. Kerg, et al., Advances in Neural Information Processing Systems 33 (2020)\\n78. J.B. Cordonnier, A. Loukas, M. Jaggi, International Conference on Learning Repre-\\nsentations (2020)\\n79. Z. Lin, et al., International Conference on Learning Representations (2017)\\n80. R. Paulus, C. Xiong, R. Socher, International Conference on Learning Representations\\n(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)\\n82. D. Povey, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), IEEE pp. 5874–5878 (2018)\\n83. A. Vyas, et al., Advances in Neural Information Processing Systems 33 (2020)\\n84. W. Chan, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), Shanghai pp. 4960—-4964 (2016)\\n85. M. Sperber, et al., In proceedings of Annual Conference of the International Speech\\nCommunication Association (InterSpeech) pp. 3723–3727 (2018)\\n86. L. Kaiser, et al., arXiv preprint arXiv:1706.05137 (2017)\\n87. C. Xu, et al., Proceedings of the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)\\n89. P. Ramachandran, et al., Advances in Neural Information Processing Systems 32 pp.\\n68–80 (2019)\\n90. Y. Li, et al., International Conference on Machine Learning (2019)\\n91. I. Goodfellow, et al., Advances in Neural Information Processing Systems 27 pp. 2672–\\n2680 (2014)\\n92. H. Zhang, et al., International Conference on Machine Learning pp. 7354–7363 (2019)\\n93. T. Xu, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) pp. 1316–1324 (2018)\\n94. A. Yu, et al., International Conference on Learning Representations (2018)\\n95. J. Zhang, et al., Conference on Uncertainty in Artiﬁcial Intelligence (2018)\\n96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)\\n98. J. Du, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing pp. 2216–2225 (2018)\\n99. X. Li, et al., Advances in Neural Information Processing Systems 33 (2020)\\n100. B. Yang, et al., AAAI Conference on Artiﬁcial Intelligence 33, 387 (2019)\\n101. B. Yang, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium pp. 4449–4458 (2018)\\n102. Proceedings of the IEEE International Conference on Computer Vision pp. 3286–3295\\n103. P. Shaw, J. Uszkoreit, A. Vaswani, Proceedings of NAACL-HLT, New Orleans,\\nLouisiana pp. 464–468 (2018)\\n104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial\\nIntelligence, (IJCAI-18) pp. 4345–4352 (2018)\\n106. H. Le, T. Tran, S. Venkatesh, International Conference on Machine Learning (2020)\\n107. T. Shen, et al., International Conference on Learning Representations (2018)\\n108. S. Bhojanapalli, et al., International Conference on Machine Learning (2020)\\n109. Y. Tay, et al., International Conference on Machine Learning (2020)\\n110. S. Sukhbaatar, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 331–335 (2019)\\n111. Y. Jernite, et al., International Conference on Learning Representations (2017)\\n112. R. Shu, H. Nakayama, In Proceedings of the First Workshop on Neural Machine Trans-\\nlation, Vancouver, Canada pp. 1–10 (2017)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 20}, page_content='Attention Mechanism in Neural Networks:\\n21\\n113. J. Hao, et al., Proceedings of NAACL-HLT, Minneapolis, Minnesota pp. 1198–1207\\n(2019)\\n114. X. Huang, et al., International Conference on Machine Learning (2020)\\n115. V. Shiv, C. Quirk, Advances in Neural Information Processing Systems 32 pp. 12,081–\\n12,091 (2019)\\n116. Z. Li, et al., International Conference on Machine Learning (2020)\\n117. Y. Hoshen, Advances in Neural Information Processing Systems 30, Long Beach, CA,\\nUSA (2017)\\n118. S. Hu, et al., International Conference on Learning Representations (2021)\\n119. E. Parisotto, R. Salakhutdinov, International Conference on Learning Representations\\n(2021)\\n120. S. Wu, et al., Advances in Neural Information Processing Systems 33 (2020)\\n121. A. Bosselut, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)\\n123. K. Choi, et al., International Conference on Machine Learning (2020)\\n124. C. Doersch, A. Gupta, A. Zisserman, Advances in Neural Information Processing Sys-\\ntems 33 pp. 21,981–21,993 (2020)\\n125. N. Carion, et al., European Conference on Computer Vision pp. 213—-229 (2020)\\n126. X. Zhu, et al., International Conference on Learning Representations (2021)\\n127. X. Liu, et al., International Conference on Machine Learning pp. 6327–6335 (2020)\\n128. J. Kasai, et al., International Conference on Machine Learning (2020)\\n129. D. Hudson, L. Zitnick, International Conference on Machine Learning pp. 4487–4499\\n(2021)\\n130. A. Radford, et al., Technical Report, OpenAI (2018)\\n131. A. Radford, et al., OpenAI blog p. 9 (2019)\\n132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)\\n134. Z. Dai, et al., Proceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics pp. 2978–2988 (2019)\\n135. E. Parisotto, International Conference on Machine Learning (2020)\\n136. X. Ma, et al., Advances in Neural Information Processing Systems 32 pp. 2232–2242\\n(2019)\\n137. L. Lathauwer, SIAM Journal on Matrix Analysis and Applications 30(3), 1033 (2008)\\n138. L. Tucker, Psychometrika 31(3), 279 (1966)\\n139. J. Devlin, et al., Proceedings of NAACL-HLT 2019 pp. 4171–4186 (2019)\\n140. W. Taylor, Journalism Bulletin 30(4), 415 (1953)\\n141. K. Clark, et al., arXiv preprint arXiv:1906.04341 (2019)\\n142. S. Sun, et al., Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)\\n143. W. Wang, et al., Advances in Neural Information Processing Systems 33 (2020)\\n144. J. McCarley, R. Chakravarti, A. Sil, arXiv preprint arXiv:1910.06360 (2020)\\n145. O. Zafrir, et al., The 5th Workshop on Energy Eﬃcient Machine Learning and Cogni-\\ntive Computing - NeurIPS (2019)\\n146. M. Joshi, et al., In Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing pp. 5803–5808 (2019)\\n147. L. Gong, et al., International Conference on Machine Learning pp. 2337–2346 (2019)\\n148. Z. Lan, et al., International Conference on Learning Representations (2020)\\n149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)\\n151. M. Joshi, et al., Transactions of the Association for Computational Linguistics 8, 64\\n(2020)\\n152. Y. Liu, et al., arXiv preprint arXiv:1907.11692 (2019)\\n153. P. He, et al., International Conference on Learning Representations (2021)\\n154. V. Sanh, et al., the 5th Workshop on Energy Eﬃcient Machine Learning and Cognitive\\nComputing - NeurIPS (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 21}, page_content='22\\nDerya Soydaner\\n155. W. Wang, et al., International Conference on Learning Representations (2020)\\n156. S. Shen, et al., AAAI Conference on Artiﬁcial Intelligence 34, 8815 (2020)\\n157. J. Lee, et al., Bioinformatics 36(4), 1234 (2020)\\n158. P. Prakash, et al., AAAI Conference on Artiﬁcial Intelligence 35, 453 (2021)\\n159. Z. Wu, et al., International Conference on Learning Representations (2020)\\n160. S. Mehta, et al., International Conference on Learning Representations (2021)\\n161. Y. Tay, et al., International Conference on Learning Representations (2021)\\n162. S. Yun, et al., International Conference on Learning Representations (2018)\\n163. Y. Rong, et al., Advances in Neural Information Processing Systems 33 (2020)\\n164. J. Yang, et al., Advances in Neural Information Processing Systems 34 (2021)\\n165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)\\n167. X. Fan, et al., Advances in Neural Information Processing Systems 33 (2020)\\n168. G. Brunner, et al., International Conference on Learning Representations (2020)\\n169. A. Dosovitskiy, et al., International Conference on Learning Representations (2021)\\n170. A. Katharopoulos, et al., International Conference on Machine Learning (2020)\\n171. L. Greengard, V. Rokhlin, Journal of Computational Physics 73(2), 325– (1987)\\n172. T. Nguyen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n173. N. Kitaev, L. Kaiser, A. Levskaya, International Conference on Learning Representa-\\ntions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)\\n176. R. Child, et al., arXiv preprint arXiv:1904.10509 (2019)\\n177. G. Correia, V. Niculae, A. Martins, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing pp. 2174–2184 (2019)\\n178. H. Peng, et al., International Conference on Learning Representations (2021)\\n179. Y. Chen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n180. M. Zaheer, et al., Advances in Neural Information Processing Systems 33 (2020)\\n181. C.Z. Huang, et al., International Conference on Learning Representations (2019)\\n182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer\\nVision pp. 377–386 (2021)\\n184. C. Zhu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n185. A. Jaegle, et al., International Conference on Machine Learning pp. 4651–4664 (2021)\\n186. K. Choromanski, et al., International Conference on Learning Representations (2021)\\n187. A. El-Nouby, et al., Advances in Neural Information Processing Systems 34 (2021)\\n188. Q. Yu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n189. Z. Zeng, et al., International Conference on Machine Learning pp. 12,321–12,332 (2021)\\n190. Z. Shen, et al., Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision pp. 3531–3539 (2021)\\n191. S. Luo, et al., Advances in Neural Information Processing Systems 34 (2021)\\n192. X. Ma, et al., Advances in Neural Information Processing Systems 34 (2021)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 0}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilities.\\nBuilt upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁed multi-task frame-\\nwork comprising specialized data transformation and training strategies.\\nThe\\ndata transformation scheme enables the incorporation of more diverse textual\\ntraining datasets, while the task-speciﬁc training strategies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline leveraging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentation, and Hard negative\\nexample generation to improve the semantic richness and sample diﬃculty of\\nthe training set. Additionally, we employ a two-stage training strategy, compris-\\ning initial retrieval-focused pretraining followed by full-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robust retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more diverse data is crucial for\\nadvancing retrieval model performance, and that leveraging LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructions on GitHub2.\\n1\\nIntroduction\\nText embedding models, which transform natural language text into mathematical vec-\\ntor representations, play an indispensable role in text mining, question-answering sys-\\ntems, recommendation systems, and retrieval-augmented generation. Recently, LLM-\\nbased agent technology has experienced rapid development and widespread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhanced agent systems\\n∗https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 1}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nin terms of real-time performance, long-term memory, data privacy preservation, and\\nknowledge integration capabilities. With the continuous advancement of neural net-\\nworks and deep learning, text embeddings have evolved from early sparse representa-\\ntions (e.g., BM25[1]) to dense representations based on ﬁne-tuned deep networks such\\nas BERT[2] and T5[3], leading to signiﬁcant performance improvements[4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatGPT[9], ushered in\\na new era of text embeddings based on LLM representations, including models like text-\\nembedding-3-large and RepLLaMA[10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For instance, to address\\nthe limitation of decoder-only architectures—where causal attention mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—several approaches have\\nbeen proposed: Echo Embedding[11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semantics. LLM2Vec[12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dependency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction.\\nAnother widely adopted approach is knowledge distillation,\\nwhere text embeddings are treated as the ”signal states” representing textual seman-\\ntics. By distilling knowledge from high-performing teacher models to student models,\\nthe objective is to optimize the embedding performance. For instance, Jasper[14] em-\\nploys a multi-stage knowledge distillation framework, combining with multiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debater[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, iteratively optimizing doc-\\nument representations through continuous COT. Distillation is applied to constrain\\nthe ﬁnal token representation to learn the optimal semantic states from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for model optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to diminishing gra-\\ndient norms during optimization. Thus they developed an asynchronous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refreshes the negative\\nsample pool using the current model parameters, thereby ensuring the maintenance\\nof up-to-date and optimally challenging negative samples. Both Conan-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering mechanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerful Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed multi-task learn-\\ning framework that not only accommodates more diverse training data but also bring\\neﬃcient learning across three key tasks: retrieval, natural language inference (NLI),\\nand classiﬁcation. Our framework comprises two core components: 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc requirements of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extraction from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s characteristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and generalization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 2}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ntor representation, we propose a data synthesis method by employing three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building upon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling batch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a two-phase train-\\ning approach, through the ﬁrst-stage retrieval training and second-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilities, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state-of-the-art av-\\nerage scores on CMTEB[22] and MTEB[23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness of our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematically coordi-\\nnates both data processing and training pipelines, enhancing diversity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, including Para-\\nphrasing, Data augmentation, and Hard negative generation.\\nThese methods\\nsigniﬁcantly enhance the quality of training corpora, thereby improving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval performance; and\\nstage 2 implements balanced training with controled retrieval/non-retrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pair classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintaining re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTEB and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed methods.\\n2\\nRelated Works\\n2.1\\nText Embedding Models\\nText vector representation is a fundamental research area in natural language processing\\n(NLP) and serves as the cornerstone for language understanding. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[25], BM25[26], and LSA[27]. With\\nthe advent of pretrained language models, dense contextualized representations based\\non architectures like BERT[2] and T5[3] became widely studied and applied[4][5][6]. In\\nthe era of large language models (LLMs), major advancements have led to the devel-\\nopment of LLM-based embedding models, such as text-embedding-3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—such\\nas RoPE positional encoding[35], RMSNorm[36], and GeGLU activation[37]—combined\\nwith their strong semantic contextualization capabilities acquired through large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 3}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior performance in re-\\ntrieval and related tasks.\\n2.2\\nEmbedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrastive learning training on\\nhigh-quality labeled positive and negative samples.\\nIn unsupervised learning, early\\nwork like SimCSE[7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance the model’s dis-\\ncriminative representation capability. For weakly supervised learning, gte[33] utilized\\nlarge-scale structured data (web search data, title-article pairs, etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval training data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fewer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to optimize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tuning, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀerent tasks. Piccolo2[39]\\nintroduced multi-task hybrid loss functions for diverse downstream tasks, an approach\\nwe also incorporate.\\nSFR-Embedding[30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimination. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categories from the per-\\nspective of circle loss[40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3\\nData Synthesis\\nData quantity and quality are the most critical factors in model optimization, data\\nsynthesis methods have become a critical research direction due to the high cost of\\nmanual annotation.\\nDoc2Query[41] and Query2Doc[42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents respectively, enhancing data\\nfor improved RAG performance.\\nPromptagator[43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot demonstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varying intents or distri-\\nbutions.\\nGPL[44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieval models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unnatural Instructions[45]\\nleverages prompt and in-context learning (ICL) techniques to generate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experimental results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-speciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 4}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n2.4\\nHard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive learning for retrieval model\\ntraining. Early work like ANCE[46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint states to maintain\\noptimally challenging samples. Conan-Embedding[24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refreshing samples when\\ntheir scores fall below a threshold. NV-Retriever[47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering criteria to minimize\\nfalse negatives. LGAI-Embedding[17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁltering to eliminate\\nfalse negatives.\\n3\\nUniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including retrieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them collectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-wise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[39], SFR-Embedding[30], NV-Embed[47], Conan-Embedding[24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses\\n(InfoNCE[48], Cosent[49], etc.).\\nOur design principle aims to accommodate more tasks and data types, enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capabilities. We propose\\na uniﬁed multi-task learning framework that categorizes training data into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into embedding training data\\nthrough this framework. The following sections detail the framework’s components and\\nimplementation methods.\\n3.1\\nModel Architecture\\nEmbedding models based on BERT or T5 [39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirectional attention mech-\\nanisms. However, recent large language models predominantly adopt decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constraining tokens’ ability to\\ncapture contextual information. Several studies have addressed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoint due to its exceptional\\nChinese language contextual capabilities. Consequently, we implemented the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-directional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 5}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The model architecture is\\nshown in Figure 1\\n3.2\\nData Transformation\\n3.2.1\\nRetrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper and QA datasets.\\nGiven the heterogeneous nature of these datasets across domains and purposes, we\\ndesign a retrieval-oriented data transformation methodology to convert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their processing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transformation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is applied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commonly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets generally contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 6}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted into a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits remarkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question paired with one\\nanswer) represents the most suitable format for retrieval training. For transfor-\\nmation, the ”Question/Query/User” portion is converted into queries, while the\\n”Answer/Response/Assistant” portion is processed as documents.\\n3.2.2\\nNLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment, and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) and textual entailment\\ntasks as illustrative examples. Our approach distinctively reformulates NLI tasks into\\ntext pair-score formats compatible with Cosent loss[49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationships. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric se-\\nmantic matching to determine whether two sentences share equivalent meaning.\\nSTS datasets typically consist of sentence pairs with associated labels, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical scores (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical value of 1, while\\n”no”/”false” are converted to 0. The data is then structured into (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each single original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neutral, contradic-\\ntion.\\nOur processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contradiction respec-\\ntively. We construct (query, document, score) triplets accordingly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3\\nCLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the same category\\nexhibit semantic proximity while distinct boundaries separate diﬀerent classes. NV-\\nEmbed[47] compared label-based and example-based data construction methods, with\\nexperimental results demonstrating the superiority of the latter. Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by using the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 7}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, and selecting texts\\nfrom diﬀerent labels as negative examples.\\nFigure 2 provides a detailed schematic\\nillustration of this process.\\n3.3\\nTraining Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized training objectives to\\nto enhance model training eﬃciency.\\nThis section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1\\nRetrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[48], but incorporate an\\nimprovement inspired by gte[33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a batch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is explicitly described\\nin Equation (1).\\nLRetrieval = −1\\nn\\nX\\ni\\nlog\\nesim(qi,d+\\ni )/τ\\nesim(qi,d+\\ni )/τ + P\\nj esim(qi,d−\\nj )/τ + P\\nj̸=i esim(qi,qj)/τ\\n(1)\\n3.3.2\\nNLI\\nFor NLI tasks, the transformed labels are numerically comparable and exhibit ordinal\\nrelationships.\\nWe employ Cosent loss[49] to optimize such data, which is designed\\nbased on the principles of Circle loss[40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demonstrating faster\\nconvergence. Its mathematical formulation is presented in Equation (2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 8}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nLNLI = log(1 +\\nX\\nsim(i,j)>sim(k,l)\\nexp(sim(xk, xl) −sim(xi, xj)\\nτ\\n))\\n(2)\\n3.3.3\\nCLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However, since CLS data is\\nprocessed in an example-based manner, directly applying in-batch negative sampling\\non classiﬁcation datasets with limited categories may lead to false negatives from items\\nof diﬀerent classes.\\nNumerous studies have proposed diverse approaches to address\\nthis issue[51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling, for each negative\\nsample from other data instances, we check whether its label matches the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remains InfoNCE, with\\nthe CLS loss formulation shown in Equation (3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = −1\\nn\\nX\\ni\\nlog esim(ti,t+\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t+\\ni )/τ +\\nX\\nn\\nMASK(ti, t−\\ni,n) · esim(ti,t−\\ni,n)/τ+\\nX\\nj̸=i\\nMASK(ti, tj) · esim(ti,tj)/τ+\\nX\\nj̸=i\\nX\\nn\\nMASK(ti, t−\\nj,n) · esim(ti,t−\\nj,n)/τ\\nand Cti = Ct+\\ni\\nand MASK(ti, tj) =\\n(\\n0\\nif Cti = Ctj,\\n1\\notherwise\\n4\\nData Synthesis\\nThe production of higher-quality data through data production has gained critical im-\\nportance in embedding training.\\nManual annotation incurs higher costs and lower\\nproduction eﬃciency, thus developing eﬀective automated data synthesis methods has\\nemerged as a key research focus. Recent advancements in large language models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accurate interpretation\\nof human instructions and generation of high-quality outputs. Multiple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 9}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nleverages LLM capabilities for data production across three dimensions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃculty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The constraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1\\nStructural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and grammatical features,\\nwhich represent relatively surface-level characteristics reﬂecting word arrangements,\\ncombinations, tenses, voices, and other formal attributes.\\nEmbedding models must\\naccurately capture underlying semantics despite variations in surface form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantically equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to structural variations\\nwhile accurately capturing semantic information, we propose a Paraphrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented instances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure 3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2\\nSemantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcations yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure uniform vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphrasing, we propose an\\naugmentation method using LLM to diversify semantics. The core concept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 10}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the domain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, and viewpoints while\\nremaining contextually anchored. This process is governed via prompt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3\\nMore challenging embeddings\\nHard negative examples are crucial for enhancing the performance of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, the framework is\\nillustrated in Figure 5.\\nDuring Data paraphrasing and Augmentation, we implement task-speciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs and add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by randomly duplicating\\nexisting entries containing the original sentences and replacing them with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 11}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 6: Training pipeline\\n5\\nTraining Optimization\\n5.1\\nData Grouping Strategy\\nPrior works like Linq-Embedding[52] and SFR-Embedding-Mistral[30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixing them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointers recorded to\\nenable sequential reading in subsequent iterations. For sampling weights, we adopt\\nthe data sampling strategy from gte[33] and mgte[50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation (4)\\npi =\\nlα\\ni\\nPm\\nj=1 lα\\nj\\n(4)\\n5.2\\nTwo-Stage Training\\nInspired by NV-Embed’s[47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusively uses retrieval-\\noriented training data, while the second stage integrates both retrieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Data Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating additional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 12}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nfunction to control the proportion of retrieval training, ensuring that throughout the\\nsecond training stage, the computational contribution of retrieval data accounts for η,\\nwhile non-retrieval data constitutes 1−η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling ratio determination. Let\\nthe training data D = [d1, d2, ..., dN] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [l1, l2, ..., lN]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scaling factor α, a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets for summation.\\nThe equations are as follows:\\nSret =\\nX\\ni\\nMi · lα\\ni\\nSnon ret =\\nX\\ni\\n(1 −Mi) · lα\\ni\\nwhere Mi =\\n(\\n0\\nif di ∈RET,\\n1\\nelse\\nwhere RET denotes the set of retrieval training datasets. The retrieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [lsamp\\n1\\n, lsamp\\n2\\n, ...lsamp\\nN\\n]\\nwhere lsamp\\ni\\n=\\n(ηRET ·lα\\ni\\nSret\\nif di ∈RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6\\nExperiments\\n6.1\\nTraining Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-multilingual-gemma2-\\ndata 3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[28],\\nEcho Embedding[11], and LLM2Vec[12], is also incorporated.\\nThe aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[64], Natural Questions (NQ)[65], ELI5[66], HotpotQA[67],\\nMIRACL[68], SQuAD[69], FEVER[70], Quora Question Pairs(QQP), and DuReader[71],\\netc.\\nPrevious researchers have already systematically collected and organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such as Huatuo medical QA6,\\nall above data has been incorporated. Additional data from huggingface’s sentence-\\ntransformers7 repository includes reddit, hover[72], mr-tydi[73], law-gpt, and s2orc[74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 13}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nOther sources encompass web questions, BioASQ[54], cmrc[55], CSL8, nli for simcse\\n(used in SimCSE[7] and GTE[33]), MLDR9, GLUE Benchmark[56], Yelp Reviews[57]\\nand Weibo Sentiment10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb-Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test sets.\\nFor data requiring format conversion, we apply the methodologies described in Sen-\\ntion 3.2.\\nDatasets with limited samples (e.g., subsets of bge and e5 series, Imdb-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultimately obtained ap-\\nproximately 5M high-quality training samples through API interfaces. We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores using GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic hard negative\\ngeneration. Due to API cost constraints, only 30% of hard negatives are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3-1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal training dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2\\nTrainset Instructions\\nFor most training data containing instruction formats, we retain their original con-\\ntents. For the MTEB training set, we adopt instructions corresponding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external data lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-speciﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix A.2.\\n6.3\\nTraining Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm-up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the second stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k steps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considering data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the absence of forward\\ncomputation for negative samples, the batch size is set to 768. Across all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine temperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Grouping Strategy\\nremains unchanged between the two stages, except that the second stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 14}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all stages to ensure\\nmaximum performance improvement. The query and passage lengths are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoPE[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations for all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem\\nStage1\\nStage2\\nWarm-up\\n300\\nSteps\\n3e-5\\n2e-5\\nLR\\n32k\\n8k\\nBatch Size InfoNCE\\n256\\nBatch Size Cosent\\n-\\n768\\nPrecision\\nbﬂoat16\\nTemperature\\n0.02\\nOptimizer\\nAdam\\nQuery Length\\n256\\nPassage Length\\n1536\\n6.4\\nCompared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MTEB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For MTEB, the compar-\\native models include LGAI-Embedding-Preview[17], the Seed series (v1.5[75] , v1.6[38]),\\nQwen series (8B, 4B)[34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[30],\\nand NV-Embed-v2[47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[39].\\n6.5\\nMain Results\\nThis section presents the evaluation results of Qzhou-embedding on MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranked models. As detailed\\nin Table 2, Table 3, Qzhou-embedding achieves state-of-the-art performance across\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach.\\nFurthermore, under MTEB’s oﬃcial ranking protocol, Qzhou-embedding\\nsecured the top position on both leaderboards. (Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the listed models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/classiﬁcation\\nbenchmark where the top score does not appear in the top 10 models.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 15}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel\\nClass.\\nClust.\\nPair Class.\\nRerank.\\nSTS\\nRetr.\\nSumm.\\nMean(Task)\\nMean(TaskType)\\nLGAI-Embedding-Preview\\n89.97\\n59.25\\n88.67\\n49.13\\n66.18\\n86.69\\n38.93\\n74.12\\n68.4\\nSeed1.5-Embedding\\n89.88\\n60.83\\n87.39\\n50.67\\n67.45\\n87.23\\n36.44\\n74.76\\n68.56\\nQwen3-Embedding-8B\\n90.43\\n58.57\\n87.52\\n51.56\\n69.44\\n88.58\\n34.83\\n75.22\\n68.71\\nQwen3-Embedding-4B\\n89.84\\n57.51\\n87.01\\n50.76\\n68.46\\n88.72\\n34.39\\n74.6\\n68.1\\nSeed1.6-embedding\\n92.42\\n59.22\\n85.07\\n50.28\\n64.9\\n86.87\\n37.1\\n74.07\\n67.98\\ngemini-embedding-001\\n90.05\\n59.39\\n87.7\\n48.59\\n64.35\\n85.29\\n38.28\\n73.3\\n67.67\\njasper en vision language v1\\n90.27\\n60.52\\n88.14\\n50\\n56.05\\n84.37\\n37.19\\n71.41\\n66.65\\nLinq-Embed-Mistral\\n83\\n54.07\\n88.44\\n49.44\\n60.14\\n84.69\\n37.26\\n69.8\\n65.29\\nSFR-Embedding-Mistral\\n80.47\\n54.93\\n88.59\\n50.15\\n59.33\\n84.77\\n36.32\\n69.31\\n64.94\\nNV-Embed-v2\\n87.19\\n47.66\\n88.69\\n49.61\\n62.84\\n83.82\\n35.21\\n69.81\\n65\\nQZhou-Embedding(Ours)\\n88.97\\n61.65\\n92.43\\n51.77\\n67.12\\n91.65\\n33.05\\n75.97\\n69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel\\nClass.\\nClust.\\nPair Class.\\nRerank.\\nSTS\\nRetr.\\nMean(Task)\\nMean(TaskType)\\nSeed1.6-embedding\\n77.98\\n73.11\\n88.71\\n71.65\\n79.69\\n68.94\\n75.63\\n76.68\\nSeed1.5-Embedding\\n79.37\\n71.11\\n89.57\\n70.14\\n79.33\\n66.56\\n74.87\\n76.01\\nritrieve zh v1\\n76.88\\n66.5\\n85.98\\n72.86\\n76.97\\n63.92\\n72.71\\n73.85\\nConan-embedding-v2\\n76.47\\n68.84\\n92.44\\n74.41\\n78.31\\n65.48\\n74.24\\n75.99\\nxiaobu-embedding-v2\\n76.53\\n65.17\\n85.94\\n72.58\\n76.49\\n64.18\\n72.36\\n73.48\\nQwen3-Embedding-8B\\n76.97\\n80.08\\n84.23\\n66.99\\n78.21\\n63.53\\n73.84\\n75\\nConan-embedding-v1\\n76.77\\n66.33\\n85.68\\n72.76\\n76.67\\n63.67\\n72.5\\n73.65\\nzpoint large embedding zh\\n76.4\\n62.23\\n85.75\\n72.33\\n76.36\\n63.86\\n71.81\\n72.82\\npiccolo-large-zh-v2\\n76.42\\n62.16\\n85.22\\n70\\n74.36\\n63.46\\n70.86\\n71.94\\nQwen3-Embedding-4B\\n75.46\\n77.89\\n83.34\\n66.05\\n77.03\\n61.26\\n72.27\\n73.51\\nQZhou-Embedding(Ours)\\n79.99\\n70.91\\n95.07\\n74.85\\n78.80\\n71.89\\n76.99\\n78.58\\n7\\nConclusion\\nIn this technical report, we present QZhou-Embedding, a general-purpose contextual\\ntext embedding model with exceptional text representation capabilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transformation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we developed a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques such as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a two-stage training\\nstrategy comprising initial retrieval-focused training followed by full-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robust retrieval per-\\nformance.\\nThe model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings establish that data qual-\\nity and diversity are pivotal for improving embedding model capabilities. In the future,\\nwe will focus on developing multimodal and multilingual embedding models, as well\\nas exploring eﬀective applications of embedding models in agent systems, aiming to\\nintegrate cutting-edge technologies to optimize this classical module.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 16}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conference on Research and\\nDevelopment in Information Retrieval, organised by Dublin City University, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine learning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-supervised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert:\\nSentence embeddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conference on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics.\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large dual encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-shot learners.”\\nAdvances in neural information processing systems 33 (2020): 1877-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”Fine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” arXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large language models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.05961 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 17}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jasper and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv:2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective representa-\\ntions for dense retrieval through deliberate thinking before search.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical report[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:2007.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved techniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405.17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:2407.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu Lian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embeddings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ıc Magne, and Nils Reimers. ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embedding:\\nGen-\\neral text embedding with more and better negative samples.” arXiv preprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–idf measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Conference on Research\\nand Development in Information Retrieval, organised by Dublin City University,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 18}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and\\nFuru Wei. Improving text embeddings with large language models. arXiv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with transfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Representations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learners. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage contrastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embedding.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer normalization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer,\\nNoam.\\n”Glu\\nvariants\\nimprove\\ntransformer.”\\narXiv\\npreprint\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” arXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarity op-\\ntimization.” In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 19}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with\\nlarge language models. In Proceedings of the 2023 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapore. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fewshot dense\\nretrieval from 8 examples. In The Eleventh International Conference on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of dense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, pages\\n2345–2360, Seattle, United States. Association for Computational Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unnatural in-\\nstructions: Tuning language models with (almost) no human labor.” arXiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:2007.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large language\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevating text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 20}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competition[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chinese machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark and analysis\\nplatform for natural language understanding[J]. arXiv preprint arXiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sentiment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association for computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural language understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Proceedings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computational linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAWS-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation.” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated machine read-\\ning comprehension dataset. In Proceedings of the Workshop on Cognitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-located with the\\n30th Annual Conference on Neural Information Processing Systems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 21}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,\\net al. Natural questions: a benchmark for question answering research. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019.\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and\\nMichael Auli. 2019. ELI5:\\nLong Form Question Answering. In Proceedings of\\nthe 57th Annual Meeting of the Association for Computational Linguistics, pages\\n3558–3567, Florence, Italy. Association for Computational Linguistics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2023.\\n[69] Pranav\\nRajpurkar,\\nJian\\nZhang,\\nKonstantin\\nLopyrev,\\nand\\nPercy\\nLiang.\\nSquad:\\n100,000+ questions for machine comprehension of text. arXiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁcation. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang.\\n2018. DuReader: a Chinese Machine Reading Comprehension Dataset from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Reading for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association for Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extraction And Claim\\nVeriﬁcation. In Findings of the Association for Computational Linguistics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Linguistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark for dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, pages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 22}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar\\nNaim, Gustavo Hernandez ´ Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding:\\nGeneralizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA\\nAppendix\\nA.1\\nFramework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem\\nExplanation\\nKeep core semantics\\nPreserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within±15%\\nThe length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language\\nThe language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld\\nThe content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 23}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nA.2\\nInstruction Examples\\nTable 5: Instruction for partial training data\\nDataset\\nInstruction\\nHuatuo\\nGiven a medical question, retrieve user replies that\\nbest answer the question\\nReddit\\nRetrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT\\nRetrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI\\nRetrieve semantically similar text\\nYelp\\nClassify the customer review of businesses\\nWeibo\\nClassify the sentiment of Weibo comments\\nA.3\\nData Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, followed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery\\npos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 24}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery\\npos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury.\\nBy the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 25}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 8: Augmentation Example\\nquery\\npos\\nneg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire\\na\\nrefundable\\nde-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards\\nmay\\naccept\\nlower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks\\noﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By\\nthe\\ntime\\nof\\nEliz-\\nabethan literature a vig-\\norous\\nliterary\\nculture\\nin\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The\\nFaerie\\nQueene’,\\nan\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater\\neclipsed\\nby\\nShake-\\nspeare’s popularity.\\nChristopher\\nMarlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork\\nit\\nwasn’t\\nan\\nepic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical\\nthemes\\nthrough\\nblank\\nverse\\nand\\nbecame\\na\\ncornerstone\\nof\\nEnglish\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso\\ndealt\\nwith\\nreligious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 26}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery\\npos\\nneg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli\\nexpansion\\nduring\\nthe\\nArab-Israeli\\nconﬂicts,\\nthough\\nhis\\nwarnings\\nto\\nNasser\\nwere delayed\\nand\\ninitially\\ndismissed,\\nwhile\\nother Arab leaders focused\\nmore\\non\\ndirect\\nmilitary\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand\\nmixed\\nwith\\nbroader\\nregional\\ntensions,\\nwhile\\nEgyptian\\nmilitary\\nmove-\\nments in Sinai were already\\nunderway\\nunder\\nAmer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 0}, page_content='Title: A Survey of Object Detection: From Region Proposals to End-to-End \\nTransformers \\nAuthor: [Your Name/Institution] Date: October 2025 \\n \\nAbstract \\nObject detection, a fundamental task in computer vision, involves identifying and localizing \\ninstances of objects within an image or video. It goes beyond simple image classification by not only \\ndetermining the class of an object but also providing a bounding box that precisely outlines its \\nlocation. This paper provides a comprehensive survey of the evolution of object detection \\nmethodologies, primarily focusing on the deep learning era. We begin by contextualizing the \\nproblem with a brief overview of traditional computer vision techniques. The core of the review is \\ndedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy \\nthrough a region proposal mechanism; and single-stage detectors, such as YOLO and SSD, which \\noptimize for speed by performing detection in a single pass. We then explore key architectural \\ncomponents like backbone networks, anchor boxes, and non-maximum suppression. The survey \\nculminates with a discussion of modern architectures, including the paradigm-shifting DETR \\n(DEtection TRansformer), which reframes object detection as an end-to-end set prediction problem. \\nFinally, we cover standard evaluation metrics, common datasets, real-world applications, and the \\nongoing challenges and future directions that are shaping the field. \\nKeywords: Object Detection, Computer Vision, Deep Learning, R-CNN, YOLO, SSD, Transformer, \\nDETR, Bounding Box, mAP.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 1}, page_content='Table of Contents \\n1. Introduction 1.1. Defining Object Detection: Classification and Localization 1.2. Distinction \\nfrom Other Vision Tasks 1.3. The Importance of Object Detection 1.4. Paper Structure \\n2. Background and Foundational Concepts 2.1. Traditional Computer Vision Approaches \\n(Viola-Jones, HOG) 2.2. The Sliding Window Method 2.3. The Deep Learning Revolution \\n3. Two-Stage Object Detectors: A Focus on Accuracy 3.1. The \"Propose, then Classify\" \\nParadigm 3.2. R-CNN: Regions with CNN Features 3.3. Fast R-CNN: Sharing Computation \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\n4. Single-Stage Object Detectors: A Focus on Speed 4.1. The \"Single Pass\" Paradigm 4.2. \\nYOLO: You Only Look Once 4.3. SSD: Single Shot MultiBox Detector \\n5. Key Architectural Components and Innovations 5.1. Backbone Networks: The Feature \\nExtractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections \\n6. Modern Architectures and the Rise of Transformers 6.1. Balancing Speed and Accuracy: \\nEfficientDet 6.2. DETR: End-to-End Object Detection with Transformers \\n7. Evaluation Metrics and Datasets 7.1. Intersection over Union (IoU) 7.2. Average Precision \\n(AP) and mean Average Precision (mAP) 7.3. Landmark Datasets (PASCAL VOC, COCO) \\n8. Applications and Real-World Use Cases 8.1. Autonomous Vehicles 8.2. Medical Imaging \\n8.3. Retail and Inventory Management 8.4. Security and Surveillance \\n9. Challenges and Future Directions 9.1. Detecting Small and Occluded Objects 9.2. The \\nSpeed vs. Accuracy Trade-off 9.3. Domain Adaptation and Generalization 9.4. Few-Shot and \\nZero-Shot Detection \\n10. Conclusion \\n11. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 2}, page_content='1. Introduction \\n1.1. Defining Object Detection: Classification and Localization \\nObject detection is a core computer vision task concerned with answering two fundamental questions \\nabout an image: \"What objects are in this image?\" and \"Where are they located?\". The first \\nquestion is a classification task, assigning a class label (e.g., \"cat,\" \"car,\" \"person\") to an object. The \\nsecond is a localization task, providing a tight-fitting bounding box (typically defined by x/y \\ncoordinates and width/height) around each identified object. \\n1.2. Distinction from Other Vision Tasks \\nIt is crucial to distinguish object detection from related tasks: \\n• \\nImage Classification: Simply assigns one label to an entire image (e.g., \"this is a picture of a \\ncat\"). \\n• \\nSemantic Segmentation: Assigns a class label to every pixel in the image but does not \\ndistinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\"). \\n• \\nInstance Segmentation: Assigns a class label to every pixel and differentiates between \\nobject instances (e.g., \"person 1,\" \"person 2,\" \"person 3\"). Object detection can be seen as a \\nprecursor to this more complex task. \\n1.3. The Importance of Object Detection \\nThe ability to detect and locate objects is foundational to how machines perceive and interact with \\nthe physical world. It is the technology that enables self-driving cars to see pedestrians and other \\nvehicles, allows doctors to identify tumors in medical scans, and helps robots navigate complex \\nenvironments. Its broad applicability has made it one of the most actively researched areas in \\nartificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-\\ndeep learning techniques. We will then delve into the two primary families of deep learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 3}, page_content='detectors: two-stage and single-stage. We will discuss their core components, modern architectures \\nincluding Transformers, and conclude with evaluation metrics, applications, and future challenges. \\n \\n2. Background and Foundational Concepts \\n2.1. Traditional Computer Vision Approaches \\nBefore deep learning, object detection relied on hand-crafted features. Methods like the Viola-Jones \\nframework (famous for real-time face detection) used simple Haar-like features and a cascade of \\nclassifiers. Other approaches used more complex feature descriptors like HOG (Histogram of \\nOriented Gradients), often paired with a classifier like a Support Vector Machine (SVM), to \\nidentify objects. These methods were effective for specific tasks but were brittle and did not \\ngeneralize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid \\nacross all possible locations and scales of an image. For each window, a feature descriptor would be \\ncomputed and fed to a classifier. This method was computationally exhaustive and prone to errors. \\n2.3. The Deep Learning Revolution \\nThe success of AlexNet in the 2012 ImageNet classification challenge marked a turning point. \\nResearchers quickly realized that the rich, hierarchical features learned automatically by \\nConvolutional Neural Networks (CNNs) were far more powerful than any hand-crafted features. \\nThis discovery paved the way for the modern era of object detection. \\n \\n3. Two-Stage Object Detectors: A Focus on Accuracy \\nTwo-stage detectors break the object detection problem into two distinct steps, a paradigm that \\ngenerally leads to higher accuracy at the cost of speed. \\n3.1. The \"Propose, then Classify\" Paradigm'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 4}, page_content=\"The core idea is to first generate a sparse set of region proposals—areas of the image that are likely \\nto contain an object. In the second stage, a classifier is run only on these proposed regions to \\ndetermine the object's class and refine the bounding box. \\n3.2. R-CNN: Regions with CNN Features \\nR-CNN was the first major breakthrough in applying deep learning to this paradigm. However, its \\nprocess was slow and cumbersome: \\n1. Generate ~2000 region proposals using an external algorithm like Selective Search. \\n2. Warp/resize each proposed region to a fixed size. \\n3. Pass each warped region independently through a pre-trained CNN to extract features. \\n4. Use a set of SVMs to classify the object in each region. \\n3.3. Fast R-CNN: Sharing Computation \\nFast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then \\nprojected onto this feature map. A novel RoI (Region of Interest) Pooling layer extracts a fixed-size \\nfeature vector from each proposed region, which is then fed into a classifier. This shared computation \\nmade the process much faster. \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\nThe bottleneck in Fast R-CNN was the external Selective Search algorithm for proposing regions. \\nFaster R-CNN introduced the Region Proposal Network (RPN), a small neural network that learns \\nto generate high-quality region proposals directly from the CNN features. By integrating the RPN, \\nFaster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed \\nSingle-stage detectors remove the region proposal step and instead perform localization and \\nclassification in a single forward pass of the network, making them extremely fast and suitable for \\nreal-time applications.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 5}, page_content='4.1. The \"Single Pass\" Paradigm \\nThese models treat object detection as a regression problem. They look at the image once and \\ndirectly predict a set of bounding boxes and their corresponding class probabilities. \\n4.2. YOLO: You Only Look Once \\nThe YOLO family of models is renowned for its speed. YOLO divides the input image into a grid. \\nFor each grid cell, the model simultaneously predicts: \\n• \\nSeveral bounding boxes. \\n• \\nA \"confidence\" score for each box, indicating how likely it is to contain an object. \\n• \\nClass probabilities for the object within the box. This unified architecture allows for end-to-\\nend training and blazingly fast inference speeds, making it ideal for video processing. \\n4.3. SSD: Single Shot MultiBox Detector \\nSSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make \\npredictions. By making predictions at different scales, SSD is much better at detecting objects of \\nvarious sizes, particularly small ones, compared to the original YOLO. \\n \\n5. Key Architectural Components and Innovations \\nModern detectors, whether two-stage or single-stage, share several common components. \\n5.1. Backbone Networks: The Feature Extractors \\nThe backbone is a deep CNN (like ResNet, VGG, or MobileNet) pre-trained on a large image \\nclassification dataset (e.g., ImageNet). Its role is to act as a powerful feature extractor, converting the \\nraw pixel data of an image into rich, hierarchical feature maps that can be used for detection. \\n5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of \\npre-defined default boxes called anchor boxes. These anchors have various sizes and aspect ratios'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 6}, page_content='and are tiled across the image at different locations. Using anchors reframes the problem from \\npredicting absolute coordinates to refining a well-placed prior, which makes learning easier for the \\nnetwork. \\n5.3. Non-Maximum Suppression (NMS): Pruning Redundant Detections \\nA detector will often output multiple, highly overlapping bounding boxes for the same object. NMS \\nis a crucial post-processing step that cleans up these redundant detections. It sorts all boxes by their \\nconfidence scores, keeps the box with the highest score, and suppresses (discards) any other boxes \\nthat have a high overlap with it. \\n \\n6. Modern Architectures and the Rise of Transformers \\n6.1. Balancing Speed and Accuracy: EfficientDet \\nThe EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion \\nmechanism (BiFPN) to achieve state-of-the-art efficiency, balancing high accuracy with low \\ncomputational cost. \\n6.2. DETR: End-to-End Object Detection with Transformers \\nDETR (DEtection TRansformer) represents a major paradigm shift. It completely eliminates the need \\nfor hand-crafted components like anchor boxes and NMS. DETR uses a standard Transformer \\nencoder-decoder architecture, similar to those used in NLP. It treats object detection as a direct set \\nprediction problem: the model ingests image features and directly outputs the final set of unique \\nobject detections. This simplifies the detection pipeline significantly and has opened up a new and \\nexciting research direction. \\n \\n7. Evaluation Metrics and Datasets \\n7.1. Intersection over Union (IoU)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 7}, page_content='IoU is the fundamental metric used to measure the \"correctness\" of a predicted bounding box. It is \\ncalculated as the area of overlap between the predicted box and the ground-truth box, divided by the \\narea of their union. A detection is typically considered a \"true positive\" if its IoU with a ground-truth \\nbox is above a certain threshold (e.g., 0.5). \\n7.2. Average Precision (AP) and mean Average Precision (mAP) \\nAverage Precision (AP) is the primary metric for evaluating the performance of a detector on a \\nsingle object class. It is calculated from the precision-recall curve and effectively measures the \\ndetector\\'s accuracy across all confidence levels. Mean Average Precision (mAP) is the average of \\nthe AP values across all object classes and is the standard metric for comparing different object \\ndetection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC \\nand Microsoft COCO (Common Objects in Context). The COCO dataset, with its large number of \\nobject categories and instances per image, is the current benchmark for modern object detectors. \\n \\n8. Applications and Real-World Use Cases \\nObject detection is a deployed and impactful technology across numerous industries. \\n• \\nAutonomous Vehicles: Detecting cars, pedestrians, cyclists, and traffic signals is essential \\nfor safe navigation. \\n• \\nMedical Imaging: Assisting radiologists by automatically locating tumors, lesions, or other \\nanomalies in X-rays, CT scans, and MRIs. \\n• \\nRetail: Powering cashier-less stores, monitoring shelf inventory, and analyzing customer foot \\ntraffic. \\n• \\nSecurity and Surveillance: Automatically detecting intruders, unattended baggage, or \\nmonitoring crowd density. \\n \\n9. Challenges and Future Directions'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 8}, page_content='Despite immense progress, several challenges remain. \\n• \\nDetecting Small and Occluded Objects: Models still struggle to reliably detect objects that \\nare very small, far away, or partially hidden. \\n• \\nThe Speed vs. Accuracy Trade-off: While models are becoming more efficient, the \\nfundamental trade-off between real-time speed and maximum accuracy remains a key design \\nconsideration. \\n• \\nDomain Adaptation and Generalization: A model trained on daytime, sunny weather data \\nmay fail when deployed at night or in the rain. Improving robustness to new environments is \\na major challenge. \\n• \\nFew-Shot and Zero-Shot Detection: Training models to detect new object categories with \\nvery few (or zero) labeled examples is an active and important area of research.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 9}, page_content=\"10. Conclusion \\nObject detection has undergone a remarkable transformation, moving from slow, brittle systems \\nbased on hand-crafted features to highly accurate and efficient end-to-end deep learning models. The \\nevolution from the methodical two-stage R-CNN family to the rapid single-stage YOLO and SSD \\ndetectors, and now to the elegant, anchor-free Transformer-based models like DETR, showcases the \\nfield's rapid pace of innovation. As a core enabling technology for machine perception, object \\ndetection continues to solve critical real-world problems and will undoubtedly remain a central focus \\nof AI research for years to come. \\n \\n11. References \\n[Viola & Jones, 2001] Viola, P., & Jones, M. (2001). Rapid object detection using a boosted cascade \\nof simple features. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision \\nand Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature \\nhierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE \\nconference on computer vision and pattern recognition. \\n[Ren et al., 2015] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time \\nobject detection with region proposal networks. Advances in neural information processing systems. \\n[Redmon et al., 2016] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look \\nonce: Unified, real-time object detection. Proceedings of the IEEE conference on computer vision \\nand pattern recognition. \\n[Liu et al., 2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. \\n(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. \\n(2020). End-to-end object detection with transformers. European conference on computer vision.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 10}, page_content=''),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 0}, page_content='Title: Self-Training with Uncertainty-Aware Style Transfer for Cross-Domain \\nObject Detection \\nPrincipal Investigator: [Your Name] Affiliation: [Your Institution/Research Group] Date: October \\n15, 2025 \\n \\nAbstract \\nModern object detection models achieve remarkable performance but suffer a significant drop in \\naccuracy when deployed in environments (target domains) that differ from their training data (source \\ndomain). This problem of domain shift is a major obstacle to the real-world application of \\ntechnologies like autonomous driving, where a vehicle must operate reliably in diverse weather, \\nlighting, and geographic conditions. This proposal outlines a research project to develop a novel \\nframework for unsupervised domain adaptation in object detection. We propose a method that \\ncombines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between \\ndomains, artificially augmenting the training data. More importantly, we will enhance a self-training \\npipeline by incorporating uncertainty estimation. By using techniques like Monte Carlo Dropout, \\nour model will only leverage pseudo-labels from the target domain in which it has high confidence, \\npreventing the accumulation of errors from incorrect labels. We hypothesize that this uncertainty-\\naware approach will make the self-training process more stable and effective, leading to a significant \\nimprovement in object detection performance in unseen target domains. The proposed research will \\nbe evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems. \\nKeywords: Research Proposal, Object Detection, Domain Adaptation, Self-Training, Uncertainty \\nEstimation, Style Transfer, Autonomous Vehicles.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 1}, page_content='Table of Contents \\n1. Introduction and Problem Statement 1.1. The Success and Brittleness of Modern Detectors \\n1.2. The Challenge of Domain Shift 1.3. Research Questions and Objectives 1.4. Proposed \\nContribution \\n2. Literature Review 2.1. State-of-the-Art Object Detection Models 2.2. Unsupervised Domain \\nAdaptation (UDA) 2.3. UDA Techniques in Object Detection 2.3.1. Adversarial Training \\nMethods 2.3.2. Style Transfer and Image-to-Image Translation 2.3.3. Self-Training and \\nPseudo-Labeling \\n3. Proposed Methodology 3.1. Overall Framework Architecture 3.2. Module 1: Cross-Domain \\nStyle Transfer 3.3. Module 2: Self-Training with Pseudo-Labeling 3.4. The Core Innovation: \\nUncertainty-Aware Label Filtering \\n4. Experimental Setup and Evaluation 4.1. Datasets and Benchmarks 4.2. Baseline Models \\nfor Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact \\non Autonomous Systems and Robotics 5.3. Contribution to the Field of AI \\n6. Plan of Work and Timeline 6.1. Phase 1: Literature Review and Environment Setup \\n(Months 1-2) 6.2. Phase 2: Implementation of Core Modules (Months 3-6) 6.3. Phase 3: \\nExperimentation and Analysis (Months 7-10) 6.4. Phase 4: Dissemination of Results (Months \\n11-12) \\n7. Ethical Considerations \\n8. Conclusion \\n9. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 2}, page_content='1. Introduction and Problem Statement \\n1.1. The Success and Brittleness of Modern Detectors \\nDeep learning-based object detectors like Faster R-CNN and YOLO have become incredibly \\naccurate, forming the perception backbone for many emerging technologies. However, their success \\nis predicated on the assumption that the training and testing data are drawn from the same statistical \\ndistribution. \\n1.2. The Challenge of Domain Shift \\nIn the real world, this assumption is frequently violated. A model trained exclusively on clear, sunny \\nday driving data may fail catastrophically when deployed at night, in the rain, or in a city with \\ndifferent architecture. This phenomenon is known as domain shift. Manually annotating data for \\nevery possible domain is prohibitively expensive and unscalable. Therefore, Unsupervised Domain \\nAdaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research. \\n1.3. Research Questions and Objectives \\nThis research aims to answer the following question: How can we make an object detector robust to \\ndomain shift without requiring any labeled data from the new domain? Our primary objectives are: \\n1. To design a framework that leverages both image-level style translation and model-level self-\\ntraining. \\n2. To develop a novel mechanism to filter noisy pseudo-labels generated during self-training by \\nestimating model uncertainty. \\n3. To empirically validate the proposed framework and demonstrate its superiority over existing \\nUDA methods. \\n1.4. Proposed Contribution \\nThe main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own \\nmistakes by trusting incorrect \"pseudo-labels.\" By introducing a principled mechanism for the model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 3}, page_content='to gauge its own uncertainty, we can select only the most reliable pseudo-labels, leading to more \\nstable and effective adaptation. \\n \\n2. Literature Review \\n2.1. State-of-the-Art Object Detection Models \\nOur work will build upon established object detection architectures. We will consider both a two-\\nstage detector (e.g., Faster R-CNN) and a Transformer-based detector (e.g., DETR) as the base \\nmodels for our adaptation framework. \\n2.2. Unsupervised Domain Adaptation (UDA) \\nUDA is a well-established field in machine learning. The central goal is to leverage a label-rich \\nsource domain to learn a task in a label-scarce target domain. \\n2.3. UDA Techniques in Object Detection \\n• \\nAdversarial Training: These methods use a \"domain discriminator\" network that tries to \\ndistinguish between features from the source and target domains. The main network is then \\ntrained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features. \\n• \\nStyle Transfer: Generative models like GANs are used to translate images from the source \\nstyle to the target style (e.g., making a sunny image look foggy). This creates a synthetic \\nlabeled dataset in the target style. \\n• \\nSelf-Training: This involves using an initial model to make predictions on the unlabeled \\ntarget data. The most confident predictions are then treated as \"pseudo-labels\" and are used to \\nretrain the model. This approach is powerful but risks error accumulation if the pseudo-labels \\nare noisy. Our proposed work directly addresses this key limitation. \\n \\n3. Proposed Methodology \\n3.1. Overall Framework Architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 4}, page_content='The proposed system will consist of three interconnected modules operating on a base object \\ndetector. The model will be trained on labeled source data (e.g., sunny images) and unlabeled target \\ndata (e.g., rainy images). \\n3.2. Module 1: Cross-Domain Style Transfer \\nWe will first train a CycleGAN model to learn the mappings between the source and target domains. \\nThis will allow us to translate a source image into a \"fake\" target image (e.g., sunny -> rainy) and \\nvice-versa. This provides a basic form of data augmentation, allowing the detector to see labeled \\nimages that look like they are from the target domain. \\n3.3. Module 2: Self-Training with Pseudo-Labeling \\nIn parallel, we will use the model trained on the source data to generate predictions (bounding boxes \\nand classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering \\nThis is the central component of our proposal. Instead of naively trusting all pseudo-labels above a \\nsimple confidence threshold, we will estimate the model\\'s uncertainty for each prediction. We will \\nuse Monte Carlo Dropout, a technique where dropout is applied at inference time over multiple \\nforward passes. The variance in the resulting predictions serves as a strong indicator of model \\nuncertainty. We will then filter the pseudo-labels using a combined score of confidence and low \\nuncertainty. Only the most certain and confident pseudo-labels will be added to a replay buffer used \\nto fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks \\nWe will focus on autonomous driving scenarios. The primary experiment will be adapting from the \\nCityscapes dataset (clear weather) to the Foggy Cityscapes dataset. We will also evaluate on other \\ncommon shifts, such as adapting from synthetic data (Sim10k) to real-world data (KITTI).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 5}, page_content='4.2. Baseline Models for Comparison \\nWe will compare our method against three baselines: \\n1. A \"Lower Bound\" model trained only on source data. \\n2. A state-of-the-art adversarial training method for UDA. \\n3. A standard self-training method without uncertainty awareness. \\n4.3. Evaluation Metrics \\nPerformance will be measured using the standard object detection metric, mean Average Precision \\n(mAP), calculated on the labeled validation set of the target domain. \\n \\n5. Expected Results and Broader Impact \\n5.1. Hypothesized Performance Gains \\nWe expect our uncertainty-aware framework to significantly outperform the baselines. We \\nhypothesize that by reducing the noise in the pseudo-labeling process, our model will adapt more \\neffectively, resulting in a 5-10% absolute improvement in mAP on the target domain compared to \\nstandard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for \\nautonomous vehicles, drones, and industrial robots. This research could help bridge the gap between \\ndevelopment and real-world deployment of these technologies. \\n \\n6. Plan of Work and Timeline \\nThe project is planned for a 12-month period. \\n• \\nPhase 1 (Months 1-2): In-depth literature review; setting up the computational environment \\nand baseline models.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 6}, page_content='• \\nPhase 2 (Months 3-6): Implementation of the style transfer module and the uncertainty-\\naware self-training loop. \\n• \\nPhase 3 (Months 7-10): Conducting extensive experiments on benchmark datasets, \\nanalyzing results, and performing ablation studies. \\n• \\nPhase 4 (Months 11-12): Writing a research paper for submission to a top-tier computer \\nvision conference (e.g., CVPR, ICCV) and finalizing the project report. \\n \\n7. Ethical Considerations \\nThe primary application of this research is to enhance safety in autonomous systems. However, \\nobject detection technology can also be used for surveillance. Our research will be conducted \\ntransparently, and we will focus our evaluation on publicly available datasets related to driving. We \\nwill not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion \\nThis research proposal addresses the critical problem of domain shift in object detection. By \\nproposing a novel framework that integrates style transfer with a more robust, uncertainty-aware \\nself-training mechanism, we aim to significantly advance the state of the art in unsupervised domain \\nadaptation. The successful completion of this project will produce more reliable perception models, \\nthereby accelerating the safe and responsible deployment of AI in real-world applications. \\n \\n9. References \\n[Hoffman et al., 2018] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K., ... & \\nDarrell, T. (2018). CyCADA: Cycle-consistent adversarial domain adaptation. International \\nconference on machine learning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 7}, page_content='[Saito et al., 2017] Saito, K., Watanabe, K., Ushiku, Y., & Harada, T. (2017). Asymmetric tri-\\ntraining for unsupervised domain adaptation. International conference on machine learning. \\n[Gal & Ghahramani, 2016] Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: \\nRepresenting model uncertainty in deep learning. International conference on machine learning. \\n[Zou et al., 2018] Zou, Y., Yu, Z., Kumar, B., & Wang, J. (2018). Unsupervised domain adaptation \\nfor semantic segmentation via class-balanced self-training. European conference on computer vision.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PDF Loader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader , PyMuPDFLoader\n",
    "\n",
    "## load all text files from the directory\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf\" , \n",
    "    glob=\"*.pdf\", ## Pattren to match files \n",
    "    loader_cls= PyMuPDFLoader ,  # the loader class to use \n",
    ")\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78f88481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_PDFSearch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
