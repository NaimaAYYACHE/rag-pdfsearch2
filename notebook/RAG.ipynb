{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d2b9ea",
   "metadata": {},
   "source": [
    "# ðŸ“– Project Introduction\n",
    "\n",
    "This project implements a **Retrieval-Augmented Generation (RAG) system** tailored for PDF documents.  \n",
    "The main goal is to allow users to **ask natural language questions** and receive **accurate, context-aware answers** based on the content of their documents.  \n",
    "\n",
    "The pipeline consists of three main stages:  \n",
    "1. **Data Ingestion** â€“ extract and prepare text from PDFs.  \n",
    "2. **Retrieval** â€“ find the most relevant document chunks using semantic embeddings.  \n",
    "3. **Generation** â€“ use a Large Language Model (LLM) to produce clear answers from retrieved context.  \n",
    "The system is designed to be **modular, scalable, and transparent**, preserving metadata and sources for every answer.\n",
    "\n",
    "![Data ingestion](../images/ragdiagram-ezgif.com-resize.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09205393",
   "metadata": {},
   "source": [
    "# ðŸ“¥ Step 1 â€” Data Ingestion\n",
    "\n",
    "Data ingestion is the **first stage of any RAG pipeline**.  \n",
    "Here we simply take raw data (PDFs, text files, etc.), **extract the text**, and **prepare it** for the next steps like chunking, embedding, and retrieval.\n",
    "\n",
    "This step builds the **foundation** of your entire RAG system â€” if ingestion is clean, everything after becomes easier.\n",
    "\n",
    "ðŸ“¸ *Data Ingestion Overview Diagram*\n",
    "\n",
    "![Data ingestion](../images/RAGWorkflow.CGeNNxJK_3wGKo.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056d3f5",
   "metadata": {},
   "source": [
    "### Part 1: Import Libraries ðŸ“š\n",
    "In this part, we import the necessary Python libraries to read PDFs, handle paths, and split text into chunks for RAG. \n",
    "\n",
    "- `os` â†’ for working with directories and paths  \n",
    "- `Path` â†’ easier handling of paths  \n",
    "- `PyPDFLoader` / `PyMuPDFLoader` â†’ load PDF files  \n",
    "- `RecursiveCharacterTextSplitter` â†’ split large texts into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d01dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 â€” Import Libraries ðŸ“š\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain community loaders for PDFs\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "# Text splitter to break documents into smaller chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df92e1",
   "metadata": {},
   "source": [
    "### Part 2: Define PDF Directory & List Files ðŸ“‚\n",
    "We set the folder where all PDFs are stored and then list all PDF files. \n",
    "\n",
    "This helps us process multiple PDFs automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd2d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files:\n",
      " - attention.pdf\n",
      " - embeddings.pdf\n",
      " - objectdetection.pdf\n",
      " - proposal.pdf\n"
     ]
    }
   ],
   "source": [
    "# Set the folder where all PDFs are stored\n",
    "pdf_directory = \"../data/pdf\"  # the path where we have the  PDF folder \n",
    "\n",
    "# Use Path to easily handle directory and list all PDFs recursively\n",
    "pdf_dir = Path(pdf_directory)\n",
    "\n",
    "# '**/*.pdf' finds all PDF files in the directory and subdirectories\n",
    "pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "# Print how many PDFs we found\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "\n",
    "# Print each PDF file name\n",
    "for f in pdf_files:\n",
    "    print(f\" - {f.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c431c24",
   "metadata": {},
   "source": [
    "### Part 3: Load PDFs into Documents ðŸ“„\n",
    "Each PDF can have multiple pages. Here we use `PyPDFLoader` (or `PyMuPDFLoader`) to read every page.\n",
    "\n",
    "We also add metadata to each page so later we know which PDF it came from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cfa057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading attention.pdf ...\n",
      "  âœ“ Loaded 22 pages from attention.pdf\n",
      "\n",
      "Loading embeddings.pdf ...\n",
      "  âœ“ Loaded 27 pages from embeddings.pdf\n",
      "\n",
      "Loading objectdetection.pdf ...\n",
      "  âœ“ Loaded 11 pages from objectdetection.pdf\n",
      "\n",
      "Loading proposal.pdf ...\n",
      "  âœ“ Loaded 8 pages from proposal.pdf\n",
      "\n",
      "Total pages loaded from all PDFs: 68\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a list to store all loaded documents\n",
    "all_documents = []\n",
    "\n",
    "# Loop through each PDF file\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"\\nLoading {pdf_file.name} ...\")  # status update\n",
    "    \n",
    "    try:\n",
    "        # Create a PDF loader for the current file\n",
    "        # You can switch to PyMuPDFLoader for faster loading\n",
    "        loader = PyPDFLoader(str(pdf_file))  \n",
    "        \n",
    "        # Load all pages from PDF into Document objects\n",
    "        documents = loader.load()  # returns a list of Document objects\n",
    "        \n",
    "        # Add extra metadata to each page/document\n",
    "        # This helps track which PDF each chunk came from\n",
    "        for doc in documents:\n",
    "            doc.metadata['source_file'] = pdf_file.name  # original PDF file\n",
    "            doc.metadata['file_type'] = 'pdf'            # type of file\n",
    "        \n",
    "        # Add the loaded pages to our main list\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"  âœ“ Loaded {len(documents)} pages from {pdf_file.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If any PDF fails to load, print error but continue\n",
    "        print(f\"  âœ— Error loading {pdf_file.name}: {e}\")\n",
    "\n",
    "# Final status\n",
    "print(f\"\\nTotal pages loaded from all PDFs: {len(all_documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607df119",
   "metadata": {},
   "source": [
    "### Part 4: Split Documents into Chunks âœ‚ï¸\n",
    "#### Recursive Text Chunking âœ‚ï¸\n",
    "\n",
    "- `chunk_size` â†’ how many characters per chunk  \n",
    "- `chunk_overlap` â†’ overlap between chunks for context retention  \n",
    "\n",
    "We will split all pages into smaller chunks.\n",
    "\n",
    "\n",
    "When working with large documents like PDFs, RAG models perform better if the text is split into smaller, manageable pieces called *chunks*.  \n",
    "**Recursive chunking** splits the text using a hierarchy of separators (like paragraphs, lines, or spaces), ensuring that chunks preserve context while staying under a maximum size.  \n",
    "\n",
    "![ Chunking](../images/imagep-6.png)\n",
    "\n",
    "![Recursive Chunking](../images/recursive_chunking-5b0b81807314eea256b042a5e5532be1.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 68 pages into 218 chunks\n",
      "\n",
      "Example chunk content (first 300 chars):\n",
      "Attention Mechanism in Neural Networks:\n",
      "Where it Comes and Where it Goes\n",
      "Derya Soydaner\n",
      "Received: 22 July 2021 / Accepted: 27 April 2022\n",
      "Abstract A long time ago in the machine learning literature, the idea of\n",
      "incorporating a mechanism inspired by the human visual system into neural\n",
      "networks was int ...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Part 4 â€” Split Documents into Chunks âœ‚ï¸\n",
    "\n",
    "# RAG models perform better if documents are split into smaller chunks\n",
    "chunk_size = 1000     # max characters per chunk\n",
    "chunk_overlap = 200   # overlap between chunks to maintain context\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,           # how many characters per chunk\n",
    "    chunk_overlap=chunk_overlap,     # number of overlapping characters between chunks\n",
    "    length_function=len,             # function to measure text length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # preferred separators for splitting\n",
    ")\n",
    "\n",
    "# Split all pages into chunks\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Split {len(all_documents)} pages into {len(chunks)} chunks\")\n",
    "\n",
    "# Show an example chunk\n",
    "if chunks:\n",
    "    print(\"\\nExample chunk content (first 300 chars):\")\n",
    "    print(chunks[0].page_content[:300], \"...\")\n",
    "    print(\"Metadata:\", chunks[0].metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16245f69",
   "metadata": {},
   "source": [
    "# Step 2 : Embeddings ðŸš€\n",
    "\n",
    "### Document Embeddings for RAG\n",
    "\n",
    "In this notebook, we will convert PDF/text chunks into **vector embeddings**.  \n",
    "Embeddings are numerical representations of text that capture semantic meaning. These embeddings are stored in a **vector database** for efficient similarity search.\n",
    "\n",
    "![Embedding ](../images/jGDZ0RdemtngeoDqS523gTvwfxY.png)\n",
    "\n",
    "We will use:\n",
    "- **SentenceTransformers** for generating embeddings.\n",
    "\n",
    "- **NumPy** for handling vectors.\n",
    "- **ChromaDB**  for storing and querying embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10146813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Part 1 â€” Import Libraries ðŸ“š\n",
    "# ===============================\n",
    "\n",
    "# NumPy for numerical operations on embeddings\n",
    "import numpy as np\n",
    "\n",
    "# SentenceTransformer to convert text into embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ChromaDB for vector storage and retrieval\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# UUID to generate unique IDs for each document/chunk\n",
    "import uuid\n",
    "\n",
    "# Type hints for better code readability\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# cosine similarity to check similarity between vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2 ...\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Part 2 â€” Initialize Embedding Model ðŸ¤–\n",
    "# ===============================\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding using SentenceTransformers\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the EmbeddingManager with a SentenceTransformer model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Pre-trained SentenceTransformer model name\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name} ...\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert a list of texts into embeddings\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Embeddings matrix of shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts ...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "# Initialize the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b4f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 218 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:49<00:00,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (218, 384)\n",
      "First embedding vector example:\n",
      "[-0.02570609 -0.06741273  0.04305235 -0.03098401  0.06855831  0.0532016\n",
      "  0.06542959  0.01391513  0.06434312 -0.0265669 ] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Part 3 â€” Encode Text Chunks ðŸ“\n",
    "# ===============================\n",
    "\n",
    "# Example: assuming you already have 'chunks' from 1_Ingestion notebook\n",
    "# chunks is a list of LangChain Document objects\n",
    "\n",
    "texts = [doc.page_content for doc in chunks]  # Extract text from each chunk\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "print(f\"First embedding vector example:\\n{embeddings[0][:10]} ...\")  # show first 10 numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994df09",
   "metadata": {},
   "source": [
    "### Part 3 : Vector Store: Save & Manage Embeddings ðŸ”—\n",
    "\n",
    "In this notebook, we will create a **vector store** to store the embeddings generated from our text chunks.  \n",
    "A vector store allows us to **search and retrieve relevant chunks** efficiently when we query our RAG system.  \n",
    "\n",
    "We will use **ChromaDB** for this, which is a lightweight vector database.  \n",
    "\n",
    "Key steps:  \n",
    "1. Initialize ChromaDB client  \n",
    "2. Create a collection  \n",
    "3. Add documents + embeddings  \n",
    "4. Query for testing\n",
    "\n",
    "![Vector chorma db ](../images/image-10.png)\n",
    "\n",
    "![ Vector Store ](../images/word_embeddinpg.webp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Part 1 â€” Import Libraries ðŸ“š\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import uuid  # to generate unique IDs for each chunk\n",
    "import numpy as np\n",
    "\n",
    "# ChromaDB for vector storage\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Type hints\n",
    "from typing import List, Any, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Part 4 â€” Vector Store Class ðŸª\n",
    "# ===============================\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Handles storing document embeddings in ChromaDB\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        \"\"\"\n",
    "        Initialize ChromaDB client and collection\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        docs_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            docs_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=docs_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents âœ…\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc28768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n",
      "Adding 218 documents to vector store...\n",
      "Successfully added 218 documents âœ…\n",
      "Total documents in collection: 218\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vector Store\n",
    "vector_store = VectorStore()\n",
    "\n",
    "# Add all chunks and embeddings\n",
    "vector_store.add_documents(chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6bcae",
   "metadata": {},
   "source": [
    "# Step 4 : Document Retrieval ðŸ”\n",
    "In this part, we import all the necessary libraries to handle:\n",
    "\n",
    "- File paths and directories (`os`)\n",
    "- Unique IDs for documents (`uuid`)\n",
    "- Type hints (`typing`)\n",
    "- Numerical operations (`numpy`)\n",
    "- Vector storage (`chromadb`)\n",
    "- Our previously created classes:\n",
    "    - `EmbeddingManager` (from 2_Embeddings.ipynb)\n",
    "    - `VectorStore` (from 3_VectorStore.ipynb)\n",
    "\n",
    "These libraries allow us to query our PDF embeddings efficiently.\n",
    "\n",
    "\n",
    "![Retrieval](../images/image-14p.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012adb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Part 1 â€” Import Libraries ðŸ“¦\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import chromadb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ef2df",
   "metadata": {},
   "source": [
    "The `RAGRetriever` class is responsible for **query-based retrieval**.\n",
    "\n",
    "- It takes a search query.\n",
    "- Converts it into an embedding.\n",
    "- Queries the vector store.\n",
    "- Returns the most relevant PDF chunks with metadata and similarity score.\n",
    "\n",
    "This is the core part of any RAG pipeline: **finding context for your questions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87863be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Part 2 â€” RAG Retriever Class ðŸ”\n",
    "# ===============================\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"\n",
    "    Handles query-based retrieval from the vector store.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: embedding_manager.__class__):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.01) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve the top_k most similar documents for a given query.\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Query the vector store\n",
    "        results = self.vector_store.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        # Process results\n",
    "        retrieved_docs = []\n",
    "        if results['documents'] and results['documents'][0]:\n",
    "            documents = results['documents'][0]\n",
    "            metadatas = results['metadatas'][0]\n",
    "            distances = results['distances'][0]\n",
    "            ids = results['ids'][0]\n",
    "            \n",
    "            for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                similarity_score = 1 - distance  # convert cosine distance to similarity\n",
    "                if similarity_score >= score_threshold:\n",
    "                    retrieved_docs.append({\n",
    "                        'id': doc_id,\n",
    "                        'content': document,\n",
    "                        'metadata': metadata,\n",
    "                        'similarity_score': similarity_score,\n",
    "                        'distance': distance,\n",
    "                        'rank': i + 1\n",
    "                    })\n",
    "        \n",
    "        print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "        return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1edea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x26d96d477a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Part 3 â€” Initialize RAG Retriever âš¡\n",
    "# ===============================\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store= vector_store , embedding_manager=embedding_manager)\n",
    "\n",
    "# Check the retriever\n",
    "rag_retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a108a",
   "metadata": {},
   "source": [
    "Now we test the retriever by asking a query.\n",
    "\n",
    "- `query`: The question we want to answer.\n",
    "- `top_k`: How many PDF chunks to retrieve.\n",
    "- Prints a **snippet of content**, source file, and similarity score.\n",
    "\n",
    "This ensures our **retriever works before connecting it to an LLM**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb1210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is an embedding?'\n",
      "Top K: 5, Score threshold: 0.01\n",
      "Generating embeddings for 1 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_610fb7dc_101',\n",
       "  'content': 'Embedding[34] designs a diversiï¬ed prompting strategy by assigning document-s peciï¬c\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'total_pages': 27,\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'page': 3,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'content_length': 238,\n",
       "   'doc_index': 101,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'page_label': '4',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.019948244094848633,\n",
       "  'distance': 0.9800517559051514,\n",
       "  'rank': 1}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Part 4 â€” Test Retrieval ðŸ“\n",
    "# ===============================\n",
    "\n",
    "query = \"What is an embedding?\"\n",
    "\n",
    "results = rag_retriever.retrieve(query=query, top_k=5)\n",
    "\n",
    "results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4fb6a",
   "metadata": {},
   "source": [
    "# ðŸ§  **Step 4 â€” Generation (LLM Response Creation)**\n",
    "\n",
    "In this step, we use a **Large Language Model (LLM)** to generate the final answer.\n",
    "\n",
    "After the retriever returns the most relevant chunks, the LLM:\n",
    "\n",
    "1. **Reads the retrieved context**\n",
    "2. **Understands the userâ€™s question**\n",
    "3. **Generates a clear, concise, context-aware answer**\n",
    "\n",
    "We use **Groqâ€™s Llama-3.1-8B-Instant**, a fast and efficient model optimized for retrieval-augmented workflows.\n",
    "\n",
    "The process:\n",
    "\n",
    "* Build a prompt that contains both the **context** and the **question**\n",
    "* Send it to the LLM\n",
    "* Return the generated answer to the user\n",
    "\n",
    "This completes the **RAG pipeline**:\n",
    "**Retrieve â†’ Augment â†’ Generate**.\n",
    "\n",
    "Below is the code that performs the generation.\n",
    "\n",
    "![generation ](../images/image-16.ppng.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "### itialize the Groq LLLM (set your Groq_API_Key in environment)\n",
    "# groq_api_key=os.getenv(\"GROK_API_KEY\")\n",
    "groq_api_key =\"GROK_API_KEY\"\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "#simple RAG function: retrieve context +generate response\n",
    "def rag_simple(query,retriever,llm,top_k=5):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8e014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API Key loaded:\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get your Groq API key\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"Groq API Key loaded:\")  # just to check it's loaded\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_api_key,\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a0270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is embeddings in LLM?'\n",
      "Top K: 5, Score threshold: 0.01\n",
      "Generating embeddings for 1 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of Large Language Models (LLMs), embeddings refer to mathematical vector representations of natural language text or multimodal data. These vector representations are used in various applications such as text mining, question-answering systems, recommendation systems, and retrieval-augmented generation.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"What is embeddings in LLM?\", rag_retriever, llm)\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
